{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profit Prophet: The Stock Market ML Predictor\n",
    "\n",
    "This project uses LSTM machine learning to make predictions of stock market prices.\n",
    "\n",
    "### Our Team\n",
    "\n",
    "* Daniel Oliyarnik\n",
    "* Harry van der Veen\n",
    "* Darren Sun\n",
    "* Jialu Xu\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We are trying to build a TAF enhanced LSTM to accurately predict stock market prices. We believe that stock prices can be forecasted based on their previous numerical values, but are also heavily impacted by speculation which we can capture using corprate news articles.\n",
    "\n",
    "## Stock Data\n",
    "\n",
    "We use a combination of the fundamental stock data (open price, high, low, close price) along with some calculated indicators (rsi, macd) in the form of a csv.\n",
    "\n",
    " Additionally, we use new articles related to the companies of interest.\n",
    "\n",
    "## Approach\n",
    "\n",
    "First we train an LSTM model for the fundamental data to get a forecast of the market based on pure numeric data. Then we apply a TAF to the prediction to reduce RMSE. This produces our forecast based on the numeric data.\n",
    "\n",
    "Next, we take the news data and extract embeddings to output a vector containing the relevance and estimations of stock impact. This output is combined with the fundamental stock data and pushed to another LSTM to predict future values. This provides the speculative forecast which accounts for corporate news.\n",
    "\n",
    "![Block Diagram.png](images/Block%20Diagram.png)\n",
    "\n",
    "Our LSTM model in this codebase is used for time series forecasting, particularly for what appears to be financial or stock data prediction. The implementation in lstm_model.py creates a sequential neural network with LSTM layers for capturing temporal patterns in sequential data.\n",
    "The model's architecture is configurable, allowing for multiple LSTM layers. Each LSTM cell maintains internal states (cell state and hidden state) that help it remember information over long sequences. The model uses dropout for regularization to prevent overfitting and supports different activation functions like 'tanh' or 'relu'.\n",
    "What makes this implementation interesting is that it uses stateful LSTMs, where the cell states are preserved between batches during training. This allows the model to maintain context across different batches of the same sequence. The state is reset after each epoch via the reset_model_states() method. The training process involves fitting the model without shuffling the data (important for time series) and resetting states between epochs.\n",
    "For forecasting, the ForecastEngine class extends the LSTM model, allowing it to make multi-step predictions by feeding its own predictions back into itself iteratively - a technique known as autoregressive forecasting. The model uses the last batch of training data as a starting point and then generates future predictions one step at a time, updating its input with each new prediction.\n",
    "\n",
    "## Tools\n",
    "\n",
    "There are multiple different ways to get the stock prices; Bloomberg Terminals, and OpenBB.\n",
    "\n",
    "News data can be imported from a variety of sources including Bloomberg and OpenBB, but also regular sites such as the Financial Times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloomberg API\n",
    "\n",
    "Bloomberg terminals are the defacto way to get stock information. UW also provides access to 4 of these terminals in the MC building. The API for Bloomberg requires the terminal to be running, so the API can only run on a machine with the terminal open.\n",
    "\n",
    "For this reason, we are moving away from Bloomberg API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloomberg API\n",
    "\n",
    "from xbbg import blp\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = './Data/'\n",
    "\n",
    "tickers = ['NVDA US Equity', 'AAPL US Equity']\n",
    "fields = ['High', 'Low', 'Last_Price']\n",
    "start_date = '2024-11-01'\n",
    "end_date = '2024-11-10'\n",
    "\n",
    "# This line hangs unless it is running with a Bloomberg terminal\n",
    "hist_tick_data = blp.bdh(tickers=tickers, fields=fields, start_date=start_date, end_date=end_date)\n",
    "\n",
    "filename = f'tick_data_{start_date}_to_{end_date}.csv'\n",
    "hist_tick_data.to_csv(DATA_DIR + filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenBB\n",
    "\n",
    "OpenBB is a free open-source implementation of Bloomberg's stock viewer. It can be run without any special software running in the background.\n",
    "\n",
    "The data is aggregated from multiple sources, though some data is inaccessible unless we purchase api keys from the corresponding sources\n",
    "\n",
    "### Finnhub\n",
    "\n",
    "Finnhub is a free data provider for corporate news. We can use this to import up to 3 months of news on a particular company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openbb\n",
    "openbb.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate limiter class\n",
    "# Some of the liraries used in the code are rate limited. This class can be used\n",
    "# to limit the number of requests made to the library in a given time period.\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class TokenBucket:\n",
    "    def __init__(self, tokens, refill_rate):\n",
    "        self.capacity = tokens  # Max tokens (60)\n",
    "        self.tokens = tokens    # Initial tokens\n",
    "        self.refill_rate = refill_rate  # Tokens added per second (60)\n",
    "        self.lock = threading.Lock()\n",
    "        self.last_refill = time.time()\n",
    "\n",
    "    def _refill(self):\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_refill\n",
    "        # Calculate tokens to add based on elapsed time\n",
    "        new_tokens = elapsed * self.refill_rate\n",
    "        if new_tokens > 0:\n",
    "            self.tokens = min(self.capacity, self.tokens + new_tokens)\n",
    "            self.last_refill = now\n",
    "\n",
    "    def consume(self, tokens=1):\n",
    "        with self.lock:\n",
    "            self._refill()\n",
    "            if self.tokens >= tokens:\n",
    "                self.tokens -= tokens\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>symbol</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>macdh</th>\n",
       "      <th>macds</th>\n",
       "      <th>headline</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>SP500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-01-02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.024994e+09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1108.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-01-02</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.309248e+09</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1108.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-01-05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.530258e+09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-01-05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.725876e+09</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-01-06</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>7.130872e+09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1123.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>2025-02-26</td>\n",
       "      <td>129.99</td>\n",
       "      <td>133.73</td>\n",
       "      <td>128.49</td>\n",
       "      <td>131.28</td>\n",
       "      <td>3.225538e+08</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>47.845685</td>\n",
       "      <td>-0.139424</td>\n",
       "      <td>-0.190138</td>\n",
       "      <td>0.050714</td>\n",
       "      <td>QTUM: Our Favorite Way To Invest In Quantum Co...</td>\n",
       "      <td>Invest in the Defiance Quantum ETF (QTUM) for ...</td>\n",
       "      <td>SeekingAlpha\\nFinnhub\\nSeekingAlpha\\nSeekingAl...</td>\n",
       "      <td>5956.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>239.66</td>\n",
       "      <td>242.46</td>\n",
       "      <td>237.06</td>\n",
       "      <td>237.30</td>\n",
       "      <td>4.115364e+07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>46.727976</td>\n",
       "      <td>2.034598</td>\n",
       "      <td>0.185772</td>\n",
       "      <td>1.848826</td>\n",
       "      <td>Auxier Asset Management Winter 2024 Market Com...</td>\n",
       "      <td>Auxier Focus Fund's Investor Class declined 2....</td>\n",
       "      <td>SeekingAlpha\\nSeekingAlpha\\nSeekingAlpha\\nMark...</td>\n",
       "      <td>5861.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>134.97</td>\n",
       "      <td>135.01</td>\n",
       "      <td>120.01</td>\n",
       "      <td>120.15</td>\n",
       "      <td>4.431758e+08</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>38.123629</td>\n",
       "      <td>-1.153285</td>\n",
       "      <td>-0.963199</td>\n",
       "      <td>-0.190086</td>\n",
       "      <td>Auxier Asset Management Winter 2024 Market Com...</td>\n",
       "      <td>Auxier Focus Fund's Investor Class declined 2....</td>\n",
       "      <td>SeekingAlpha\\nSeekingAlpha\\nSeekingAlpha\\nSeek...</td>\n",
       "      <td>5861.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>236.91</td>\n",
       "      <td>242.09</td>\n",
       "      <td>230.20</td>\n",
       "      <td>241.84</td>\n",
       "      <td>5.683336e+07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>53.196852</td>\n",
       "      <td>1.906182</td>\n",
       "      <td>0.045885</td>\n",
       "      <td>1.860297</td>\n",
       "      <td>The AI Smartphone Battle Of Titans: iPhone 16 ...</td>\n",
       "      <td>Apple's iPhone 16 and Samsung's Galaxy S25 mar...</td>\n",
       "      <td>SeekingAlpha\\nDowJones\\nMarketWatch\\nSeekingAl...</td>\n",
       "      <td>5954.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>118.02</td>\n",
       "      <td>125.09</td>\n",
       "      <td>116.40</td>\n",
       "      <td>124.92</td>\n",
       "      <td>3.890911e+08</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>43.429015</td>\n",
       "      <td>-1.553964</td>\n",
       "      <td>-1.091103</td>\n",
       "      <td>-0.462861</td>\n",
       "      <td>The Score: Home Depot, Nvidia, Tesla and More ...</td>\n",
       "      <td>The Score: Home Depot, Nvidia, Tesla and More ...</td>\n",
       "      <td>DowJones\\nSeekingAlpha\\nFinnhub\\nSeekingAlpha\\...</td>\n",
       "      <td>5954.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date    open    high     low   close        volume symbol  \\\n",
       "0     2004-01-02    0.39    0.39    0.38    0.38  2.024994e+09   AAPL   \n",
       "1     2004-01-02    0.20    0.20    0.19    0.19  1.309248e+09   NVDA   \n",
       "2     2004-01-05    0.38    0.40    0.38    0.40  5.530258e+09   AAPL   \n",
       "3     2004-01-05    0.20    0.20    0.19    0.20  1.725876e+09   NVDA   \n",
       "4     2004-01-06    0.40    0.40    0.39    0.40  7.130872e+09   AAPL   \n",
       "...          ...     ...     ...     ...     ...           ...    ...   \n",
       "10647 2025-02-26  129.99  133.73  128.49  131.28  3.225538e+08   NVDA   \n",
       "10648 2025-02-27  239.66  242.46  237.06  237.30  4.115364e+07   AAPL   \n",
       "10649 2025-02-27  134.97  135.01  120.01  120.15  4.431758e+08   NVDA   \n",
       "10650 2025-02-28  236.91  242.09  230.20  241.84  5.683336e+07   AAPL   \n",
       "10651 2025-02-28  118.02  125.09  116.40  124.92  3.890911e+08   NVDA   \n",
       "\n",
       "             rsi      macd     macdh     macds  \\\n",
       "0            NaN       NaN       NaN       NaN   \n",
       "1            NaN       NaN       NaN       NaN   \n",
       "2            NaN       NaN       NaN       NaN   \n",
       "3            NaN       NaN       NaN       NaN   \n",
       "4            NaN       NaN       NaN       NaN   \n",
       "...          ...       ...       ...       ...   \n",
       "10647  47.845685 -0.139424 -0.190138  0.050714   \n",
       "10648  46.727976  2.034598  0.185772  1.848826   \n",
       "10649  38.123629 -1.153285 -0.963199 -0.190086   \n",
       "10650  53.196852  1.906182  0.045885  1.860297   \n",
       "10651  43.429015 -1.553964 -1.091103 -0.462861   \n",
       "\n",
       "                                                headline  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "10647  QTUM: Our Favorite Way To Invest In Quantum Co...   \n",
       "10648  Auxier Asset Management Winter 2024 Market Com...   \n",
       "10649  Auxier Asset Management Winter 2024 Market Com...   \n",
       "10650  The AI Smartphone Battle Of Titans: iPhone 16 ...   \n",
       "10651  The Score: Home Depot, Nvidia, Tesla and More ...   \n",
       "\n",
       "                                                 summary  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "10647  Invest in the Defiance Quantum ETF (QTUM) for ...   \n",
       "10648  Auxier Focus Fund's Investor Class declined 2....   \n",
       "10649  Auxier Focus Fund's Investor Class declined 2....   \n",
       "10650  Apple's iPhone 16 and Samsung's Galaxy S25 mar...   \n",
       "10651  The Score: Home Depot, Nvidia, Tesla and More ...   \n",
       "\n",
       "                                                  source    SP500  \n",
       "0                                                    NaN  1108.48  \n",
       "1                                                    NaN  1108.48  \n",
       "2                                                    NaN  1122.22  \n",
       "3                                                    NaN  1122.22  \n",
       "4                                                    NaN  1123.67  \n",
       "...                                                  ...      ...  \n",
       "10647  SeekingAlpha\\nFinnhub\\nSeekingAlpha\\nSeekingAl...  5956.06  \n",
       "10648  SeekingAlpha\\nSeekingAlpha\\nSeekingAlpha\\nMark...  5861.57  \n",
       "10649  SeekingAlpha\\nSeekingAlpha\\nSeekingAlpha\\nSeek...  5861.57  \n",
       "10650  SeekingAlpha\\nDowJones\\nMarketWatch\\nSeekingAl...  5954.50  \n",
       "10651  DowJones\\nSeekingAlpha\\nFinnhub\\nSeekingAlpha\\...  5954.50  \n",
       "\n",
       "[10652 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OpenBB API\n",
    "\n",
    "from openbb import obb\n",
    "import finnhub\n",
    "from ftfy import fix_text\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor  # Use ProcessPoolExecutor for CPU-bound tasks\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=\"[Key removed - please add your own]\")\n",
    "obb.user.preferences.output_type = 'dataframe'\n",
    "rate_limiter = TokenBucket(tokens=60, refill_rate=60)\n",
    "\n",
    "FAST = 12\n",
    "SLOW = 26\n",
    "SIGNAL = 9\n",
    "\n",
    "MIN_POINTS = SLOW + SIGNAL - 1\n",
    "DAYS_TO_PAD = -(-(MIN_POINTS * 1.5) // 1) # Not every day has data. Round up to nearest integer\n",
    "\n",
    "def process_symbol_data(symbol, start_date, end_date):\n",
    "    # Wait until a token is available\n",
    "    while not rate_limiter.consume():\n",
    "        time.sleep(0.001)  # Avoid busy-waiting\n",
    "        \n",
    "    # Process data for a single symbol with technical indicators\n",
    "    try:\n",
    "        # Fetch OHLCV data\n",
    "        symbol_data_df = obb.equity.price.historical(symbol, start_date=start_date, end_date=end_date)\n",
    "        symbol_data_df['symbol'] = symbol  # Add symbol column\n",
    "\n",
    "        # Remove any duplicate dates\n",
    "        symbol_data_df = symbol_data_df[~symbol_data_df.index.duplicated(keep='first')] \n",
    "\n",
    "        # RSI\n",
    "        symbol_data_df = obb.technical.rsi(data=symbol_data_df, target='close', length=14, scalar=100.0, drift=1)\n",
    "        symbol_data_df.rename(columns={'close_RSI_14': 'rsi'}, inplace=True)\n",
    "\n",
    "        # MACD\n",
    "        symbol_data_df = obb.technical.macd(data=symbol_data_df, target='close', fast=FAST, slow=SLOW, signal=SIGNAL)\n",
    "        symbol_data_df.rename(columns={f'close_MACD_{str(FAST)}_{str(SLOW)}_{str(SIGNAL)}': 'macd',\n",
    "                                       f'close_MACDh_{str(FAST)}_{str(SLOW)}_{str(SIGNAL)}': 'macdh',\n",
    "                                       f'close_MACDs_{str(FAST)}_{str(SLOW)}_{str(SIGNAL)}': 'macds'}, inplace=True)\n",
    "        \n",
    "        # Convert 'date' index to regular index\n",
    "        symbol_data_df.reset_index(inplace=True)\n",
    "\n",
    "        # News\n",
    "        symbol_news = finnhub_client.company_news(symbol, _from=start_date, to=end_date)\n",
    "\n",
    "        # Fix encoding for all text fields in the raw API response\n",
    "        for article in symbol_news:\n",
    "            for text_field in ['headline', 'summary', 'source']:\n",
    "                if text_field in article and article[text_field] is not None:\n",
    "                    article[text_field] = fix_text(article[text_field])\n",
    "\n",
    "        group_column = 'date'\n",
    "        text_columns = ['headline', 'summary', 'source']\n",
    "\n",
    "        symbol_news_df = (pd.DataFrame(symbol_news)\n",
    "            .assign(datetime=lambda x: pd.to_datetime(x['datetime'], unit='s', errors=\"coerce\"))\n",
    "            .dropna(subset=[\"datetime\"])  # Remove invalid rows\n",
    "            .assign(datetime=lambda x: x[\"datetime\"].dt.strftime(\"%Y-%m-%d\"))\n",
    "            .rename(columns={'datetime': group_column})\n",
    "            [[group_column] + text_columns]\n",
    "        )\n",
    "\n",
    "        # Ensure 'date' is datetime in both DataFrames\n",
    "        symbol_data_df['date'] = pd.to_datetime(symbol_data_df['date'])\n",
    "        symbol_news_df['date'] = pd.to_datetime(symbol_news_df['date'])\n",
    "\n",
    "        # Aggregate news data to one row per date\n",
    "        symbol_news_df = symbol_news_df.groupby('date').agg({\n",
    "            'headline': lambda x: '\\n'.join(x.astype(str)),\n",
    "            'summary': lambda x: '\\n'.join(x.astype(str)),\n",
    "            'source': lambda x: '\\n'.join(x.astype(str))\n",
    "        }).reset_index()\n",
    "\n",
    "        symbol_data_df = symbol_data_df.merge(symbol_news_df, on='date', how='outer')\n",
    "\n",
    "        return symbol_data_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {symbol}: {traceback.format_exc()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def downloadStockData(symbols, start_date=None, end_date=None, parallel=True):\n",
    "    try:\n",
    "        # Fetch S&P 500 data once for all symbols\n",
    "        sp500_df = obb.equity.price.historical(\"SPX\", start_date=start_date, end_date=end_date)\n",
    "        sp500_df = sp500_df[['close']].rename(columns={'close': 'SP500'})\n",
    "        sp500_df.reset_index(inplace=True)\n",
    "        sp500_df['date'] = pd.to_datetime(sp500_df['date'])\n",
    "\n",
    "        # Process symbols in parallel or sequentially\n",
    "        if parallel:\n",
    "            with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                futures = [executor.submit(process_symbol_data, symbol, start_date, end_date) for symbol in symbols]\n",
    "                results = [f.result() for f in futures]\n",
    "        else:\n",
    "            results = [process_symbol_data(symbol, start_date, end_date) for symbol in symbols]\n",
    "\n",
    "        # Combine all symbols and merge with SP500\n",
    "        combined_df = pd.concat(results)\n",
    "\n",
    "        final_df = combined_df.merge(sp500_df, on='date', how='outer')        \n",
    "\n",
    "        # Add groupby-aware technical calculations\n",
    "        final_df = final_df.groupby('symbol', group_keys=False).apply(lambda x: x.sort_index())\n",
    "        final_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "        return final_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {traceback.format_exc()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Declare search bounds \n",
    "symbols = ['AAPL', 'NVDA']\n",
    "start_date = '1950-01-01'\n",
    "end_date = '2025-03-01'\n",
    "\n",
    "data_df = downloadStockData(symbols, start_date, end_date)\n",
    "\n",
    "data_df.to_csv('Stock Data.csv')\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessor\n",
    "\n",
    "We create an extended MinMaxScaler which includes extra headroom for future values. This class is also used to create a sliding window dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class BufferedMinMaxScaler(MinMaxScaler):\n",
    "    \"\"\"New Subclass, inheriting from MinMaxScalar, gives custom/buffered scale\"\"\"\n",
    "    def __init__(self, headroom=0.2):\n",
    "        super().__init__()\n",
    "        self.headroom = headroom\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X)\n",
    "        # 1. Store original data min/max\n",
    "        self.orig_data_min_ = X.min(axis=0)\n",
    "        self.orig_data_max_ = X.max(axis=0)\n",
    "\n",
    "        # 2. Calculate buffer-adjusted max\n",
    "        data_range = self.orig_data_max_ - self.orig_data_min_\n",
    "        self.data_max_ = self.orig_data_max_ + data_range * self.headroom\n",
    "        self.data_min_ = self.orig_data_min_  # Keep original min (for now, potentially will need to change)\n",
    "\n",
    "        # 3. Calculate parent class parameters (data_range_ is for parent class, incorporating the headroom)\n",
    "        self.data_range_ = self.data_max_ - self.data_min_\n",
    "        self.scale_ = (self.feature_range[1] - self.feature_range[0]) / self.data_range_\n",
    "        self.min_ = self.feature_range[0] - self.data_min_ * self.scale_\n",
    "        return self \n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, headroom=0.2):\n",
    "        # Its using the extra headroom by defult\n",
    "        self.scaler = BufferedMinMaxScaler(headroom=headroom)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.scaler.fit_transform(data)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "    def create_dataset(self, dataset, look_back=1, target_feature=0, forecast_horizon=1):\n",
    "        dataX, dataY = [], []\n",
    "\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            # Note the : here indicated to put into a 2D array [[1,2,3], [1,2,3], [1,2,3]]\n",
    "            input = dataset[i:(i+look_back), :] # IF there was only 1 feature, then need 0 to put into 1D\n",
    "            # Append into shape(X, 3)\n",
    "            dataX.append(input)\n",
    "            output = dataset[i + look_back, :]\n",
    "            dataY.append(output)\n",
    "\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    def trim_XY(self, dataX, dataY, batch_size):\n",
    "        # Removing any odd data depending on batch size\n",
    "        trim_size = len(dataX) - (len(dataX) % batch_size)\n",
    "        return dataX[:trim_size], dataY[:trim_size]\n",
    "\n",
    "    def invert_1d_prediction(self, pred_1d, feature_cols_num, target_feature=0):\n",
    "        # If we are looking at ALL the included features\n",
    "        if pred_1d.shape[1] == feature_cols_num:\n",
    "            inverted = self.scaler.inverse_transform(pred_1d)\n",
    "        else:\n",
    "            # pred_1d shape: (samples,1) -> what the INFERENCE output of the LSTM is\n",
    "            # (samples, num_features) -> (samples,1) keeping target column\n",
    "            padded = np.zeros((pred_1d.shape[0], feature_cols_num), dtype=np.float32)\n",
    "            # [0, 0, 1, 0]\n",
    "            # [0, 0, 1, 0] <- is essentially this\n",
    "            # [0, 0, 1, 0]\n",
    "            if pred_1d.shape[1] == 1:\n",
    "                padded[:, target_feature] = pred_1d[:, 0]\n",
    "            else:\n",
    "                padded[:, target_feature] = pred_1d[:, target_feature]\n",
    "            inverted = self.scaler.inverse_transform(padded)\n",
    "        # Return just the \"target\" column (out of all the feature columns)\n",
    "        return inverted[:, target_feature].reshape(-1,1)\n",
    "\n",
    "def filter_multi_features(dataset, stock_rows, feature_cols):\n",
    "    df_symbol = dataset[dataset['symbol'] == stock_rows].copy()\n",
    "    df_symbol[feature_cols] = df_symbol[feature_cols].fillna(0)\n",
    "    full_dataset = df_symbol[feature_cols].values.astype('float32')\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "\n",
    "The LSTM is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, InputLayer, RepeatVector, TimeDistributed\n",
    "\n",
    "class LSTMModel:\n",
    "    def __init__(self, layers, look_back, batch_size, neurons, epochs, activation, dropout, features=1, isReturnSeq=False, forecast_horizon=1):\n",
    "        self.layers = layers\n",
    "        self.isReturnSeq = isReturnSeq # should be either True or False\n",
    "        self.features = features\n",
    "        self.look_back = look_back\n",
    "        self.batch_size = batch_size\n",
    "        self.neurons = neurons\n",
    "        self.epochs = epochs\n",
    "        self.activation = activation # should be either the str 'tanh' or 'relu'\n",
    "        self.dropout = dropout\n",
    "        self.model = self._build_model(self.batch_size, forecast_horizon) \n",
    "        # What the class will fill\n",
    "        self.trainPredict = None\n",
    "\n",
    "    def _build_model(self, batch_size, forecast_horizon):\n",
    "        # SHOULD ONLY HAVE ONE MODEL IN MEMORY (TF handles the models in memory strangely)\n",
    "        # So clear_session is to clear the way tf stores/handles the models\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = Sequential()\n",
    "        # batch_input_shape (batch_size, num_steps, features)\n",
    "        model.add(InputLayer(batch_input_shape=(batch_size, self.look_back, self.features)))\n",
    "        # the more complex the data -> more neurons needed\n",
    "        if self.layers > 1:\n",
    "            for l in range(self.layers-1):\n",
    "                model.add(LSTM(self.neurons, activation=self.activation, dropout=self.dropout, stateful=True, return_sequences=True))\n",
    "            # In multi-layered the last is non-stateful ***\n",
    "            model.add(LSTM(self.neurons, activation=self.activation, dropout=self.dropout, return_sequences=self.isReturnSeq))\n",
    "        else:\n",
    "            model.add(LSTM(self.neurons, activation=self.activation, dropout=self.dropout, stateful=True, return_sequences=self.isReturnSeq))\n",
    "        model.add(Dense(9))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def reset_model_states(self):\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, LSTM) and layer.stateful:\n",
    "                layer.reset_states()\n",
    "        print('Model states reset')\n",
    "\n",
    "    def train(self, trainX, trainY):\n",
    "        self.reset_model_states()\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # Unlike CNNs, for RNNs (LSTM fitted) we do not shuffle\n",
    "            self.model.fit(trainX, trainY, \n",
    "                        epochs=1, \n",
    "                        batch_size=self.batch_size, \n",
    "                        shuffle=False, \n",
    "                        verbose=2)\n",
    "            \n",
    "            # Resetting states after each epoch for stateful LSTM\n",
    "            self.reset_model_states()\n",
    "            print(f\"Epoch {i+1}/{self.epochs} --- Completed\")\n",
    "\n",
    "    def predict(self, trainX):\n",
    "        # X is the training data\n",
    "        self.trainPredict = self.model.predict(trainX, batch_size=self.batch_size)\n",
    "        return self.trainPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Engine\n",
    "\n",
    "The Forecast Engine allows us to create longer predictions by feeding the last prediction into the input of the next one, in a rolling window fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class ForecastEngine(LSTMModel):\n",
    "    def __init__(self, trained_model, layers=None, isReturnSeq=True, features=None, look_back=None, batch_size=None, neurons=None, epochs=None, activation=None, dropout=None):\n",
    "        params = {'layers': layers,\n",
    "                'isReturnSeq' : isReturnSeq,\n",
    "                'features' : features,\n",
    "                'look_back': look_back,\n",
    "                'batch_size': batch_size,\n",
    "                'neurons': neurons,\n",
    "                'epochs': epochs,\n",
    "                'activation': activation,\n",
    "                'dropout': dropout}\n",
    "\n",
    "        # Use parameters from trained_model or default values IF specified\n",
    "        for param_name in params:\n",
    "            if params[param_name] is None:\n",
    "                params[param_name] = getattr(trained_model, param_name)\n",
    "\n",
    "        # Initialize LSTMModel with inherited parameters\n",
    "        super().__init__(layers=params['layers'],\n",
    "                         isReturnSeq=params['isReturnSeq'],\n",
    "                         features=params['features'],\n",
    "                         look_back=params['look_back'],\n",
    "                         batch_size=params['batch_size'],\n",
    "                         neurons=params['neurons'],\n",
    "                         epochs=params['epochs'],\n",
    "                         activation=params['activation'],\n",
    "                         dropout=params['dropout'])\n",
    "\n",
    "        self.trained_model = trained_model\n",
    "        # What the class will fill\n",
    "        self.futurePredictions = None\n",
    "\n",
    "    def forecast(self, start_input, steps, target_col_idx=0):\n",
    "        # Use _build_model from LSTMModel to create a new model for forecasting\n",
    "        forecast_model = self._build_model(self.batch_size, forecast_horizon=steps)  # Rebuild model for forecasting\n",
    "\n",
    "        # Set weights from the trained model\n",
    "        forecast_model.set_weights(self.trained_model.model.get_weights())\n",
    "\n",
    "        new_predictions = []\n",
    "        current_batch = start_input[-self.batch_size:]  # Get the last batch for prediction\n",
    "\n",
    "        for i in range(steps):\n",
    "            # Make a prediction for the next step\n",
    "            pred = forecast_model.predict(current_batch, batch_size=self.batch_size)\n",
    "\n",
    "            new_predictions.append(pred[-1, -1, target_col_idx])\n",
    "\n",
    "            # Get all the next infered values/timesteps, put into 3D shape\n",
    "            new_step = pred[:, -1, :].reshape(self.batch_size, 1, -1)\n",
    "\n",
    "\n",
    "              # Update of batch for next prediction step, dropping the oldest value (in look back) and\n",
    "              # adding the new infered values (newest in look back)\n",
    "            current_batch = np.concatenate([current_batch[:, 1:, :], new_step], axis=1)\n",
    "\n",
    "        # Convert predictions to a numpy array (predictions_array)\n",
    "        self.futurePredictions = np.array(new_predictions).reshape(-1, 1)\n",
    "\n",
    "        # ***** May need to manage resetting the states better, however may not be necessary \n",
    "\n",
    "        return self.futurePredictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAF Shift\n",
    "\n",
    "To apply the TAF, we calculate the smoothed error and trend factors to adjust predictions, with a grid search function to find optimal alpha, beta, and weight parameters that minimize prediction error against test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TAFShift:\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        self.alpha = alpha  # Smoothing constant for error TAF\n",
    "        self.beta = beta    # Smoothing constant for trend in TAF\n",
    "\n",
    "    def calculate_taf(self, data, predictions):\n",
    "        \"\"\"full_taf, predicted_taf\"\"\"\n",
    "        data = np.asarray(data).flatten()\n",
    "        predictions = np.asarray(predictions).flatten()\n",
    "\n",
    "        n = len(data) + len(predictions)\n",
    "\n",
    "        # Create a single array with all datapoints\n",
    "        combined_data = np.concatenate([data, predictions])\n",
    "        total_length = len(combined_data)\n",
    "\n",
    "        st = np.zeros(total_length) # Smooth error\n",
    "        tt = np.zeros(total_length) # Trend factor\n",
    "        taf_values = np.zeros(total_length)\n",
    "\n",
    "        # Using the first timestep (from data) as initial smoothed forecast\n",
    "        st[0] = combined_data[0]\n",
    "        tt[0] = combined_data[1] - combined_data[0]\n",
    "\n",
    "        taf_values = np.zeros(n)\n",
    "        \n",
    "        # Calculate TAF values for the complete dataset:\n",
    "        # https://courses.worldcampus.psu.edu/welcome/mangt515/lesson02_13.html\n",
    "        for t in range(1, n):\n",
    "            taf_values[t] = st[t-1] + tt[t-1]\n",
    "            st[t] = taf_values[t] + self.alpha*(combined_data[t] - taf_values[t])\n",
    "            tt[t] = tt[t-1] + self.beta*(taf_values[t] - taf_values[t-1] - tt[t-1])\n",
    "\n",
    "        return taf_values, taf_values[len(data):]\n",
    "\n",
    "    def apply_taf(self, historical_data, forecasted, normalize=False, weight=0.0):\n",
    "        _, predicted_taf = self.calculate_taf(historical_data, forecasted)\n",
    "        # We need to reshape due to applying flatten in calculate_taf must be in (n, 1) shape for plotting\n",
    "        predicted_taf = predicted_taf.reshape(-1, 1)\n",
    "\n",
    "        # using Robust scaling\n",
    "        if normalize:\n",
    "            median_taf = np.median(predicted_taf)\n",
    "            q1, q3 = np.percentile(predicted_taf, [22, 95])\n",
    "            iqr = q3 - q1\n",
    "            if iqr < 1e-6:  \n",
    "                iqr = np.std(predicted_taf) if np.std(predicted_taf) > 0 else 1.0\n",
    "            predicted_taf = (predicted_taf - median_taf) / iqr \n",
    "\n",
    "        adjusted_forecast = forecasted + weight * predicted_taf\n",
    "        return adjusted_forecast\n",
    "    \n",
    "\n",
    "def taf_search_test(calculate_rmse, historical_data, forecasted, test_data, normalize=False):\n",
    "    \"\"\"Function for getting optimal TAF\"\"\"\n",
    "    optimal_TAF_params = {}\n",
    "    alpha_range = np.arange(0.0, 1.0, 0.1)\n",
    "    optimal_alpha = 0\n",
    "    beta_range = np.arange(0.0, 1.0, 0.1)\n",
    "    optimal_beta = 0\n",
    "    weight_range = np.arange(0.0, 0.2, 0.01)\n",
    "    optimal_weight = 0\n",
    "\n",
    "    # Relys on the assumption that the weights and TAF parameters (alpha and beta) affect RMSE independently\n",
    "    lowest_rmse = 1000000\n",
    "    optimal_taf = TAFShift()\n",
    "    optimalTAF_forecast = None\n",
    "    for a in alpha_range:\n",
    "        for b in beta_range:\n",
    "            taf_shift = TAFShift(alpha=a, beta=b)\n",
    "            for w in weight_range:\n",
    "                adjusted_forecast = taf_shift.apply_taf(historical_data, forecasted, normalize, weight=w)\n",
    "                rmse = calculate_rmse(adjusted_forecast[:, 0], test_data)\n",
    "                if rmse < lowest_rmse:\n",
    "                    lowest_rmse = rmse\n",
    "                    optimal_alpha = a\n",
    "                    optimal_beta = b\n",
    "                    optimal_weight = w\n",
    "                    optimalTAF_forecast = adjusted_forecast\n",
    "\n",
    "    print(f\"TAF alpha: {optimal_alpha}, beta: {optimal_beta}, weight: {optimal_weight} | RMSE={lowest_rmse:.2f}\")\n",
    "\n",
    "    optimal_TAF_params[(optimal_alpha, optimal_beta, optimal_weight)] = (lowest_rmse, optimalTAF_forecast)\n",
    "    return optimal_TAF_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "This code implements time series cross-validation for forecasting models with a focus on LSTM networks. It preprocesses data, trains models, generates forecasts, and evaluates predictions using RMSE metrics, with optional TAF optimization to improve forecast accuracy through parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calculate_rmse(true_values, predicted_values):\n",
    "    return np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "\n",
    "def time_series_cross_validation(curr_dataset, model_params, forecast_horizon, initial_train_size, step_size, target_feature_col=0, OPTIMAL=False):\n",
    "    \"\"\"\n",
    "    curr_dataset: the full dataset (2D array, e.g. shape (n_samples, 1))\n",
    "    model_params: dict with keys: look_back, batch_size, epochs, headroom, dropout, etc.\n",
    "    forecast_horizon: number of points to forecast in each fold\n",
    "    initial_train_size: the initial number of samples used for training\n",
    "    step_size: number of samples to roll forward between folds\n",
    "    taf_params_list: list of tuples (alpha, beta, weight) to test\n",
    "\n",
    "    Returns: list of RMSE values, one per fold\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    curr_dataset = np.nan_to_num(curr_dataset, nan=0.0)\n",
    "    rmse_list = []\n",
    "    n = len(curr_dataset)\n",
    "    start = 0\n",
    "\n",
    "    train_data = curr_dataset[start:start+initial_train_size]\n",
    "\n",
    "    data_preprocessor = DataPreprocessor(headroom=model_params['headroom'])\n",
    "    scaled_train = data_preprocessor.fit_transform(train_data)\n",
    "\n",
    "    \n",
    "\n",
    "    dataX, dataY = data_preprocessor.create_dataset(scaled_train, \n",
    "                                                    look_back=model_params['look_back'],\n",
    "                                                    target_feature=target_feature_col)\n",
    "    if len(dataX) == 0:\n",
    "        raise ValueError(\"No training/data samples generated (dataX). Look back may be too big\")\n",
    "    dataX, dataY = data_preprocessor.trim_XY(dataX, dataY, model_params['batch_size'])\n",
    "\n",
    "    effective_train_samples = len(dataX)\n",
    "    effective_train_end = start + effective_train_samples + model_params['look_back']\n",
    "\n",
    "    test_end = effective_train_end + forecast_horizon\n",
    "    test_data = curr_dataset[effective_train_end:test_end]\n",
    "    print('test_data shape: ', test_data.shape)\n",
    "\n",
    "    print('*********** Starting New Cross-Validation ***********')\n",
    "    print(f\"Fold (Cross Validation) with train indices {start}:{effective_train_end} and test indices {effective_train_end}:{test_end}\")\n",
    "\n",
    "    # THIS IS WHAT I AM CHANGING FOR THE MULTI FEATURES\n",
    "    # *****Need to tie input layer to Model class*****\n",
    "    # trainX = np.reshape(dataX, (dataX.shape[0], dataX.shape[1], 1))\n",
    "    # NO NEED to reshape since dataX is already 3D, (samples, look_back, num_features)\n",
    "    trainX = dataX\n",
    "    print('trainX shape (After Reshape): ', trainX.shape)\n",
    "    trainY = dataY\n",
    "\n",
    "    lstm_model = LSTMModel(layers=model_params['layers'],\n",
    "                            isReturnSeq=False,\n",
    "                            features=model_params['features'],\n",
    "                            look_back=model_params['look_back'],\n",
    "                            batch_size=model_params['batch_size'],\n",
    "                            neurons=model_params['neurons'],\n",
    "                            epochs=model_params['epochs'],\n",
    "                            activation=model_params['activation'],\n",
    "                            dropout=model_params['dropout'],\n",
    "                            forecast_horizon=forecast_horizon)\n",
    "    lstm_model.train(trainX, trainY)\n",
    "    trainPredict = lstm_model.predict(trainX)\n",
    "\n",
    "    forecast_engine = ForecastEngine(trained_model=lstm_model, isReturnSeq=True)\n",
    "    forecastPredict = forecast_engine.forecast(trainX, forecast_horizon, target_feature_col)\n",
    "    print('Forecast infered data shape: ', forecastPredict.shape)\n",
    "    print(\"NaNs in forecastPredict:\", np.isnan(forecastPredict).sum())\n",
    "\n",
    "    # Invert the scaling for the forecast, train and test data\n",
    "    forecasted_inverted = data_preprocessor.invert_1d_prediction(forecastPredict, model_params['features'], target_feature_col)\n",
    "    print(\"NaNs in forecasted_inverted:\", np.isnan(forecasted_inverted).sum())\n",
    "\n",
    "    # Ensuring forecasted_inverted has the same number of rows as test_data\n",
    "    # I ran into issues with mismatch due to th batch size\n",
    "    num_test_samples = test_data.shape[0]\n",
    "    if forecasted_inverted.shape[0] > num_test_samples:\n",
    "        forecasted_inverted = forecasted_inverted[:num_test_samples]\n",
    "    elif forecasted_inverted.shape[0] < num_test_samples:\n",
    "        test_data = test_data[:forecasted_inverted.shape[0]]\n",
    "\n",
    "    train_predict_inverted = 0\n",
    "\n",
    "    rmse_taf_preTAF = calculate_rmse(forecasted_inverted[:, 0], test_data[:, target_feature_col])\n",
    "    print('pre TAF: ', rmse_taf_preTAF)\n",
    "\n",
    "    if OPTIMAL:\n",
    "        rmse_TAF_results = taf_search_test(calculate_rmse, scaled_train[:effective_train_end, target_feature_col], forecasted_inverted, test_data[:, target_feature_col], normalize=False) \n",
    "        return [data_preprocessor, lstm_model, forecast_engine], train_predict_inverted, effective_train_end, test_end, forecasted_inverted, rmse_taf_preTAF, rmse_TAF_results\n",
    "    else:\n",
    "        return [data_preprocessor, lstm_model, forecast_engine], train_predict_inverted, effective_train_end, test_end, forecasted_inverted, rmse_taf_preTAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer\n",
    "\n",
    "This code plots the forecasted predictions against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class Visualizer:\n",
    "    def __init__(self, scaler: 'BufferedMinMaxScaler', trained_model: 'LSTMModel', forecast_engine: 'ForecastEngine'):\n",
    "        \"\"\"Instance needs to be of || BufferedMinMaxScaler | LSTMModel | ForecastEngine || type\"\"\"\n",
    "        if not isinstance(scaler, BufferedMinMaxScaler):\n",
    "            raise TypeError(\"Expected an instance of BufferedMinMaxScaler.\")\n",
    "        if not isinstance(trained_model, LSTMModel):\n",
    "            raise TypeError(\"Expected an instance of LSTMModel.\")\n",
    "        if not isinstance(forecast_engine, ForecastEngine):\n",
    "            raise TypeError(\"Expected an instance of ForecastEngine.\")\n",
    "        self.scaler = scaler\n",
    "        self.trained_model = trained_model\n",
    "        self.forecast_engine = forecast_engine\n",
    "\n",
    "    def create_plot_array(self, full_length, loaction_num, value_arr):\n",
    "        plot_array = np.zeros((len(full_length), 1))\n",
    "        plot_array[loaction_num:loaction_num+len(value_arr)] = value_arr\n",
    "        return plot_array\n",
    "\n",
    "    def plot_results(self, rmse, train_predictions_inverted, train_end, test_end, future_predictions_inverted, curr_dataset, curr_system, target_col, curr_dir, TAFvars):\n",
    "        \"\"\"Both Train and Forecast must be given INVERTED, following the .predict/forecast output\"\"\"\n",
    "        # Extract parameters from the trained model\n",
    "        look_back = self.trained_model.look_back\n",
    "        batch_size = self.trained_model.batch_size\n",
    "        neurons = self.trained_model.neurons\n",
    "        epochs = self.trained_model.epochs\n",
    "        dropout = self.trained_model.dropout\n",
    "        train_predictions = self.trained_model.trainPredict\n",
    "\n",
    "        # Extract parameters from the forecasted data\n",
    "        layers = self.forecast_engine.layers\n",
    "        future_predictions = self.forecast_engine.futurePredictions\n",
    "\n",
    "        # Extract parameters from the scaler\n",
    "        headroom = self.scaler.headroom\n",
    "\n",
    "        # Create full time array for x-axis\n",
    "        full_time = np.arange(len(future_predictions_inverted))\n",
    "\n",
    "        # Create plot array for plot values\n",
    "        plot_array_forecast = self.create_plot_array(full_time, 0, future_predictions_inverted)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.plot(full_time[:], curr_dataset[train_end:test_end, target_col], color='green', linewidth=1.5, alpha=0.95, label='Given Data (TEST)')\n",
    "        plt.plot(full_time[:], plot_array_forecast[:], color='red', linestyle='--', linewidth=1.5, alpha=0.75, label='Future Predictions')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Time (Days)')\n",
    "        plt.ylabel('Value (Points)')\n",
    "        plt.title(f'{curr_system}, RMSE: {rmse:.2f} | TAF A:{TAFvars[0]} B:{TAFvars[1]} W:{TAFvars[2]}')\n",
    "        \n",
    "        plt.legend()\n",
    "\n",
    "        save_dir = f\"{curr_dir}/\"\n",
    "        \n",
    "        plt.savefig(f\"{save_dir}{curr_system}_predictions Lr_{layers} H_{headroom} N_{neurons} B_{batch_size} L_{look_back} E_{epochs} D_{dropout} (TAF A {TAFvars[0]} B {TAFvars[1]} W {TAFvars[2]}).png\")\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Saved plot for column: {curr_system}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tests\n",
    "\n",
    "This code runs the forecasting pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enabling multi-GPU useage on 1 node\n",
    "# gpu_strategy = tf.distribute.MirroredStrategy()\n",
    "# print(f\"Number of GPUs Available: {gpu_strategy.num_replicas_in_sync}\")\n",
    "\n",
    "gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "print(f\"Num GPUs Available: {gpus}\")\n",
    "#print(f\"Worker (1 task per node) {os.environ.get('SLURM_PROCID', 'N/A')} sees {len(gpus)} GPU(s).\")\n",
    "\n",
    "# ------------------------------\n",
    "#          Run Script\n",
    "# ------------------------------\n",
    "def run():\n",
    "    \n",
    "    data_dir = 'data/'\n",
    "    results_dir = 'Model/results/'\n",
    "\n",
    "    curr_dir = 'results1_long'\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    # Ensure the CSV is divided into columns named 'System1', 'System2', etc.\n",
    "    file = 'Stock Data.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(data_dir+file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file}' not found. Place it into the '/data' directory\")\n",
    "        return\n",
    "\n",
    "# ==================== Global Parameters ====================\n",
    "    feature_cols = ['open',\n",
    "                    'high',\n",
    "                    'low',\n",
    "                    'close',\n",
    "                    'volume',\n",
    "                    'rsi',\n",
    "                    'macd',\n",
    "                    'macdh',\n",
    "                    'macds']\n",
    "    target_feature_col = 3\n",
    "    features = len(feature_cols)\n",
    "    \n",
    "    # Fixed parameters\n",
    "    batch_size = 256\n",
    "    headroom = 1.0\n",
    "    dropout = 0.0\n",
    "    layers = 2\n",
    "    neurons = 100\n",
    "    activation = 'tanh'\n",
    "    # activation = 'relu' # NO Good!\n",
    "    \n",
    "    # Forecasting and dataset parameters\n",
    "    forecast_horizon = 60   # Number of future points to forecast per fold\n",
    "    initial_train_size = 4500\n",
    "    step_size = 0            # For rolling window (0 -> no rolling)\n",
    "\n",
    "    stocks = ['AAPL', 'NVDA']\n",
    "    \n",
    "\n",
    "    for stock in stocks:\n",
    "        if stock != 'AAPL':\n",
    "            continue\n",
    "\n",
    "        curr_dataset = filter_multi_features(df, stock, feature_cols)\n",
    "\n",
    "        # ==================== Bayesian Optimization Setup ====================\n",
    "        # Define the objective function for Bayesian Optimization\n",
    "        def objective(look_back, epochs):\n",
    "            look_back = int(look_back)\n",
    "            epochs = int(epochs)\n",
    "            \n",
    "            model_params = {\n",
    "                'features': features,\n",
    "                'look_back': look_back,\n",
    "                'batch_size': batch_size,\n",
    "                'epochs': epochs,\n",
    "                'headroom': headroom,\n",
    "                'dropout': dropout,\n",
    "                'layers': layers,\n",
    "                'neurons': neurons,\n",
    "                'activation': activation\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Run cross-validation (non-TAF version) and obtain RMSE.\n",
    "                # time_series_cross_validation (FALSE) is expected to return:\n",
    "                # model_components, train_predict_inverted, effective_train_end, test_end, forecasted_inverted, rmse_taf_preTAF\n",
    "                _, _, _, _, _, rmse = time_series_cross_validation(\n",
    "                    curr_dataset, model_params, forecast_horizon, initial_train_size, step_size, target_feature_col, False)\n",
    "            except Exception as e:\n",
    "                print(\"Error during evaluation:\", e)\n",
    "                traceback.print_exc()\n",
    "                rmse = 1e6\n",
    "\n",
    "            # BayesianOptimization maximizes the objective so return negative RMSE.\n",
    "            return -rmse\n",
    "\n",
    "        # *****************************\n",
    "        # Define parameter bounds\n",
    "        pbounds = {\n",
    "            'look_back': (2786, 2786),\n",
    "            'epochs': (348, 348)\n",
    "        }\n",
    "        # *****************************\n",
    "        \n",
    "        optimizer = BayesianOptimization(\n",
    "            f=objective,\n",
    "            pbounds=pbounds,\n",
    "            random_state=42  # For reproducibility\n",
    "        )\n",
    "        \n",
    "        print(\"Starting Bayesian optimization for:\", stock)\n",
    "        optimizer.maximize(\n",
    "            init_points=5,   # Number of random initialization points\n",
    "            n_iter=20        # Number of iterations for the optimization\n",
    "        )\n",
    "        \n",
    "        best_params = optimizer.max['params']\n",
    "        optimal_look_back = int(best_params['look_back'])\n",
    "        optimal_epochs = int(best_params['epochs'])\n",
    "        print(f\"Optimal parameters for {stock}: look_back={optimal_look_back}, epochs={optimal_epochs}\")\n",
    "        \n",
    "        # ==================== Run Cross-Validation with Optimal Parameters ====================\n",
    "        optimal_model_params = {\n",
    "            'features': features,\n",
    "            'look_back': optimal_look_back,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': optimal_epochs,\n",
    "            'headroom': headroom,\n",
    "            'dropout': dropout,\n",
    "            'layers': layers,\n",
    "            'neurons': neurons,\n",
    "            'activation': activation\n",
    "        }\n",
    "        \n",
    "        model, train_data_inverted, train_end, test_end, non_taf_forecast, rmse_non_taf, rmse_TAFs = time_series_cross_validation(curr_dataset, optimal_model_params, forecast_horizon, initial_train_size, step_size, target_feature_col, True)\n",
    "        \n",
    "        visualizer = Visualizer(scaler=model[0].scaler,\n",
    "                                trained_model=model[1],\n",
    "                                forecast_engine=model[2])\n",
    "        visualizer.plot_results(rmse_non_taf, train_data_inverted, train_end, test_end, non_taf_forecast, curr_dataset, stock, target_feature_col, results_dir+curr_dir, [0, 0, 0])\n",
    "        print(\"|=====================================|\")\n",
    "        print(\"Cross-Validation RMSEs (Non-TAF):\", rmse_non_taf)\n",
    "        print(\"|=====================================|\")\n",
    "        for (alpha, beta, weight), (rmse_taf, adjusted_forecast) in rmse_TAFs.items():\n",
    "            visualizer.plot_results(rmse_taf, train_data_inverted, train_end, test_end, adjusted_forecast, curr_dataset, stock, target_feature_col, results_dir+curr_dir, [alpha, beta, weight])\n",
    "            print(\"|=====================================|\")\n",
    "            print(\"Cross-Validation RMSEs (TAF):\", rmse_taf)\n",
    "            print(\"|=====================================|\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google import genai\n",
    "\n",
    "# --- Gemini API Embedding Function ---\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")  # Remember to censor KEY!!!!\n",
    "\n",
    "def get_article_embedding(article_text):\n",
    "    \"\"\"\n",
    "    Uses the Gemini API to embed the article text.\n",
    "    Returns a numpy array representing the embedding.\n",
    "    \"\"\"\n",
    "    result = client.models.embed_content(\n",
    "        model=\"gemini-embedding-exp-03-07\",\n",
    "        contents=article_text\n",
    "    )\n",
    "    return np.array(result.embeddings)\n",
    "\n",
    "\n",
    "def generate_target_label(article_text):\n",
    "    \"\"\"\n",
    "    Generates a target label for an article based on simple heuristics.\n",
    "    Returns:\n",
    "        label: A list in the format [relevance, up_weight, down_weight, unchanged_weight]\n",
    "    \"\"\"\n",
    "    text = article_text.lower()\n",
    "    \n",
    "    relevance_terms = [\n",
    "        # High Confidence Names (1.0)\n",
    "        (\"apple inc.\", 1.0),\n",
    "        (\"apple incorporated\", 1.0),\n",
    "        (\"apple computer\", 1.0),\n",
    "        (\"apple corporation\", 1.0),\n",
    "        (\"apple co.\", 1.0),\n",
    "        (\"apple headquarters\", 1.0),\n",
    "        (\"cupertino-based company\", 1.0),\n",
    "\n",
    "        # Stock Ticker\n",
    "        (\"aapl\", 0.5),\n",
    "        (\"aapl shares\", 0.75),\n",
    "        (\"aapl stock\", 0.75),\n",
    "        (\"nasdaq:aapl\", 0.75),\n",
    "\n",
    "        # General Apple Mentions (0.75)\n",
    "        (\"apple\", 0.75),\n",
    "        (\"apple brand\", 0.75),\n",
    "        (\"apple products\", 0.75),\n",
    "        (\"apple ecosystem\", 0.75),\n",
    "\n",
    "        # Key People\n",
    "        (\"steve jobs\", 0.75),\n",
    "        (\"tim cook\", 0.75),\n",
    "        (\"jonny ive\", 0.75),\n",
    "        (\"phil schiller\", 0.75),\n",
    "        (\"craig federighi\", 0.75),\n",
    "        (\"luca maestri\", 0.75),\n",
    "\n",
    "        # Major Products\n",
    "        (\"iphone\", 0.75),\n",
    "        (\"iphone 15\", 0.75),\n",
    "        (\"iphone 14\", 0.75),\n",
    "        (\"iphone pro\", 0.75),\n",
    "        (\"ipad\", 0.75),\n",
    "        (\"ipad pro\", 0.75),\n",
    "        (\"ipad air\", 0.75),\n",
    "        (\"macbook\", 0.75),\n",
    "        (\"macbook pro\", 0.75),\n",
    "        (\"macbook air\", 0.75),\n",
    "        (\"mac studio\", 0.75),\n",
    "        (\"mac pro\", 0.75),\n",
    "        (\"mac mini\", 0.75),\n",
    "        (\"mac os\", 0.75),\n",
    "        (\"macos\", 0.75),\n",
    "\n",
    "        # Services\n",
    "        (\"apple music\", 0.75),\n",
    "        (\"apple tv\", 0.75),\n",
    "        (\"apple tv+\", 0.75),\n",
    "        (\"apple arcade\", 0.75),\n",
    "        (\"apple news\", 0.75),\n",
    "        (\"apple fitness+\", 0.75),\n",
    "        (\"icloud\", 0.75),\n",
    "        (\"apple id\", 0.75),\n",
    "\n",
    "        # Hardware\n",
    "        (\"apple watch\", 0.75),\n",
    "        (\"apple pencil\", 0.75),\n",
    "        (\"apple silicon\", 0.75),\n",
    "        (\"m1 chip\", 0.75),\n",
    "        (\"m2 chip\", 0.75),\n",
    "        (\"m3 chip\", 0.75),\n",
    "        (\"t2 chip\", 0.75),\n",
    "        (\"homepod\", 0.75),\n",
    "        (\"airpods\", 0.75),\n",
    "        (\"airpods pro\", 0.75),\n",
    "        (\"airpods max\", 0.75),\n",
    "        (\"vision pro\", 0.75),\n",
    "        (\"apple glasses\", 0.75),\n",
    "\n",
    "        # Operating Systems\n",
    "        (\"ios\", 0.75),\n",
    "        (\"ios 17\", 0.75),\n",
    "        (\"ipados\", 0.75),\n",
    "        (\"macos ventura\", 0.75),\n",
    "        (\"macos sonoma\", 0.75),\n",
    "        (\"watchos\", 0.75),\n",
    "        (\"tvos\", 0.75),\n",
    "\n",
    "        # Financials and Market\n",
    "        (\"apple stock\", 0.75),\n",
    "        (\"apple shares\", 0.75),\n",
    "        (\"apple earnings\", 0.75),\n",
    "        (\"apple revenue\", 0.75),\n",
    "        (\"apple profits\", 0.75),\n",
    "        (\"apple forecast\", 0.75),\n",
    "        (\"apple quarterly results\", 0.75),\n",
    "        (\"apple guidance\", 0.75),\n",
    "        (\"apple investors\", 0.75),\n",
    "        (\"apple market cap\", 0.75),\n",
    "        (\"apple dividends\", 0.75),\n",
    "        (\"apple buybacks\", 0.75),\n",
    "\n",
    "        # News & Events\n",
    "        (\"apple event\", 0.75),\n",
    "        (\"apple keynote\", 0.75),\n",
    "        (\"wwdc\", 0.75),\n",
    "        (\"apple launch\", 0.75),\n",
    "        (\"spring event\", 0.75),\n",
    "        (\"fall event\", 0.75),\n",
    "\n",
    "        # Legal & Regulatory\n",
    "        (\"apple lawsuit\", 0.75),\n",
    "        (\"apple antitrust\", 0.75),\n",
    "        (\"apple vs epic\", 0.75),\n",
    "        (\"apple regulation\", 0.75),\n",
    "        (\"apple privacy\", 0.75),\n",
    "\n",
    "        # Business Activities\n",
    "        (\"apple acquisition\", 0.75),\n",
    "        (\"apple partnership\", 0.75),\n",
    "        (\"apple investment\", 0.75),\n",
    "        (\"apple r&d\", 0.75),\n",
    "        (\"apple supply chain\", 0.75),\n",
    "        (\"apple manufacturing\", 0.75),\n",
    "        (\"foxconn\", 0.75),\n",
    "        (\"apple retail\", 0.75),\n",
    "        (\"apple online store\", 0.75),\n",
    "\n",
    "        # Technology & Innovation\n",
    "        (\"apple innovation\", 0.75),\n",
    "        (\"apple ai\", 0.75),\n",
    "        (\"apple machine learning\", 0.75),\n",
    "        (\"apple chips\", 0.75),\n",
    "        (\"apple ar\", 0.75),\n",
    "        (\"apple vr\", 0.75),\n",
    "        (\"apple car\", 0.75),\n",
    "        (\"project titan\", 0.75),\n",
    "        (\"apple security\", 0.75),\n",
    "        (\"apple encryption\", 0.75),\n",
    "\n",
    "        # Software & Apps\n",
    "        (\"apple store\", 0.75),\n",
    "        (\"app store\", 0.75),\n",
    "        (\"apple developer\", 0.75),\n",
    "        (\"xcode\", 0.75),\n",
    "        (\"apple sdk\", 0.75),\n",
    "        (\"testflight\", 0.75),\n",
    "        (\"apple beta\", 0.75),\n",
    "\n",
    "        # Medium Relevance (0.5)\n",
    "        (\"smartphone market\", 0.5),\n",
    "        (\"consumer electronics\", 0.5),\n",
    "        (\"wearables\", 0.5),\n",
    "        (\"tech giant\", 0.5),\n",
    "        (\"big tech\", 0.5),\n",
    "        (\"us tech stock\", 0.5),\n",
    "        (\"silicon valley\", 0.5),\n",
    "        (\"hardware company\", 0.5),\n",
    "        (\"tablet market\", 0.5),\n",
    "        (\"smartwatch sales\", 0.5),\n",
    "        (\"mobile os\", 0.5),\n",
    "        (\"voice assistant\", 0.5),\n",
    "        (\"supply chain disruption\", 0.5),\n",
    "\n",
    "        # Low Relevance (0.25)\n",
    "        (\"consumer trends\", 0.25),\n",
    "        (\"technology adoption\", 0.25),\n",
    "        (\"semiconductor trends\", 0.25),\n",
    "        (\"software update\", 0.25),\n",
    "        (\"ai assistant\", 0.25),\n",
    "        (\"smart home\", 0.25),\n",
    "        (\"us markets\", 0.25),\n",
    "        (\"tech stocks\", 0.25),\n",
    "        (\"electronics retail\", 0.25),\n",
    "        (\"app monetization\", 0.25),\n",
    "        (\"digital marketplace\", 0.25),\n",
    "        (\"cloud storage\", 0.25),\n",
    "        (\"eco-friendly tech\", 0.25),\n",
    "\n",
    "        # Irrelevant (0.0)\n",
    "        (\"banana\", 0.0),\n",
    "        (\"orange fruit\", 0.0),\n",
    "        (\"pineapple\", 0.0),\n",
    "        (\"fruit basket\", 0.0),\n",
    "        (\"orchard\", 0.0),\n",
    "        (\"apple pie\", 0.0),\n",
    "        (\"apple cider\", 0.0),\n",
    "        (\"fruit nutrition\", 0.0),\n",
    "        (\"apple picking\", 0.0),\n",
    "        (\"red delicious\", 0.0),\n",
    "    ]\n",
    "\n",
    "    # --- Relevance Score ---\n",
    "    relevance = 0.0  # Default relevance\n",
    "    for term, score in relevance_terms:\n",
    "        if term in text:\n",
    "            relevance = max(relevance, score)\n",
    "\n",
    "    # --- Directional Weights ---\n",
    "    # Define keywords for directional sentiment related to business performance.\n",
    "    up_terms = [\n",
    "        \"growth\", \"profit\", \"increase\", \"rise\", \"gain\", \"surge\", \"expansion\",\n",
    "        \"record\", \"strong\", \"improved\", \"bullish\", \"beat\", \"outperform\", \"upgrade\"\n",
    "    ]\n",
    "    down_terms = [\n",
    "        \"loss\", \"decline\", \"drop\", \"fall\", \"decrease\", \"slump\", \"weak\",\n",
    "        \"bearish\", \"downgrade\", \"miss\", \"underperform\", \"cut\", \"reduction\"\n",
    "    ]\n",
    "    unchanged_terms = [\n",
    "        \"stable\", \"steady\", \"unchanged\", \"flat\", \"consistent\", \"no change\", \"sideways\"\n",
    "    ]\n",
    "    \n",
    "    # Count occurrences for each category\n",
    "    def count_terms(text, terms):\n",
    "        count = 0\n",
    "        for term in terms:\n",
    "            count += text.count(term)\n",
    "        return count\n",
    "\n",
    "    up_count = count_terms(text, up_terms)\n",
    "    down_count = count_terms(text, down_terms)\n",
    "    unchanged_count = count_terms(text, unchanged_terms)\n",
    "    \n",
    "    total = up_count + down_count + unchanged_count\n",
    "    if total == 0:\n",
    "        # If no keywords found, default to a neutral distribution\n",
    "        up_weight, down_weight, unchanged_weight = 0.0, 0.0, 1.0\n",
    "    else:\n",
    "        up_weight = up_count / total\n",
    "        down_weight = down_count / total\n",
    "        unchanged_weight = unchanged_count / total\n",
    "\n",
    "    # Return the label in the format: [relevance, up, down, unchanged]\n",
    "    return [relevance, up_weight, down_weight, unchanged_weight]\n",
    "\n",
    "\n",
    "class ArticleProcessor:\n",
    "    def __init__(self, excel_path, sheet_name=None):\n",
    "        \"\"\"\n",
    "        Loads the Excel file into a DataFrame.\n",
    "        Assumes the first column is a date and subsequent columns contain article text.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "\n",
    "    import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ArticleWeightPredictor:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Constructs a neural network mapping embeddings to four outputs:\n",
    "           - Relevance score (0 to 1)\n",
    "           - Direction weights (stock up, stock down, stock unchanged) that sum to 1\n",
    "        \"\"\"\n",
    "        input_layer = layers.Input(shape=(self.input_dim,))\n",
    "        # Shared hidden layers\n",
    "        x = layers.Dense(128, activation='relu')(input_layer)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        relevance = layers.Dense(1, activation='sigmoid', name='relevance')(x)\n",
    "        \n",
    "        direction_logits = layers.Dense(3, name='direction_logits')(x)\n",
    "        # Using softmax so the 3 ebeddings from the layer will add up to 1\n",
    "        direction = layers.Activation('softmax', name='direction')(direction_logits)\n",
    "        \n",
    "        output = layers.Concatenate(name='output')([relevance, direction])\n",
    "        \n",
    "        model = models.Model(inputs=input_layer, outputs=output)\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def train(self, x, y, epochs=50, batch_size=32, validation_split=0.2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          - X: Array of embeddings (shape: [num_samples, input_dim])\n",
    "          - y: Array of target outputs (shape: [num_samples, 4])\n",
    "               where the first element is the relevance score, and the next three are directional weights.\n",
    "        \"\"\"\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=validation_split, random_state=42)\n",
    "        history = self.model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3)\n",
    "            ]\n",
    "        )\n",
    "        return history\n",
    "    \n",
    "    def predict(self, embedding):\n",
    "        \"\"\"Output is 4 dimensional\"\"\"\n",
    "        return self.model.predict(np.array([embedding]))\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        self.model.save(filepath)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, input_dim):\n",
    "        predictor = cls(input_dim)\n",
    "        predictor.model = tf.keras.models.load_model(filepath)\n",
    "        return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.article_weigher import ArticleWeightPredictor\n",
    "from src.article_processor import ArticleProcessor, get_article_embedding\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, excel_path, model_filepath, sheet_name=None):\n",
    "        self.processor = ArticleProcessor(excel_path, sheet_name)\n",
    "        self.model_filepath = model_filepath\n",
    "        self.model = None  # Will be loaded in load_model()\n",
    "    \n",
    "    def load_model(self, input_dim):\n",
    "        \"\"\"\n",
    "        Loads the saved model from disk.\n",
    "        \"\"\"\n",
    "        self.model = ArticleWeightPredictor.load_model(self.model_filepath, input_dim)\n",
    "    \n",
    "    def predict_daily(self):\n",
    "        \"\"\"\n",
    "        Processes the Excel file row by row. For each day (row), it:\n",
    "          - Loops through each article (columns 2+)\n",
    "          - Gets the embedding via the Gemini API for each article\n",
    "          - Uses the neural network to predict the 4 outputs for each article\n",
    "          - Averages the 4-dimensional outputs across all articles for that day\n",
    "        Returns:\n",
    "          - dates: List of dates (from the first column)\n",
    "          - daily_predictions: A NumPy array with one 4-d vector per day.\n",
    "        \"\"\"\n",
    "        # Use the raw DataFrame from the processor\n",
    "        data = self.processor.data\n",
    "        dates = []\n",
    "        daily_predictions = []\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            # The first column is assumed to be the date\n",
    "            date = row[0]\n",
    "            article_preds = []\n",
    "            # Loop through each article (columns 2+)\n",
    "            for article in row[1:]:\n",
    "                if isinstance(article, str) and article.strip():\n",
    "                    try:\n",
    "                        # Get the embedding for this article\n",
    "                        embedding = get_article_embedding(article)\n",
    "                        # Get the neural network's 4-d prediction\n",
    "                        pred = self.model.predict(np.array([embedding]))[0]\n",
    "                        article_preds.append(pred)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing article: {e}\")\n",
    "            # If there are predictions for the day, average them; otherwise use zeros.\n",
    "            if article_preds:\n",
    "                daily_avg = np.mean(article_preds, axis=0)\n",
    "            else:\n",
    "                daily_avg = np.zeros(4)\n",
    "            dates.append(date)\n",
    "            daily_predictions.append(daily_avg)\n",
    "        \n",
    "        return dates, np.array(daily_predictions)\n",
    "    \n",
    "    def save_predictions(self, dates, predictions, output_excel):\n",
    "        \"\"\"\n",
    "        Saves the daily predictions to an Excel file.\n",
    "        Each row in the Excel file corresponds to a day with 4 prediction values.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(predictions, columns=['relevance', 'up', 'down', 'unchanged'])\n",
    "        df.insert(0, \"date\", dates)\n",
    "        df.to_excel(output_excel, index=False)\n",
    "        print(f\"Predictions saved to {output_excel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.article_weigher import ArticleWeightPredictor\n",
    "from src.article_processor import ArticleProcessor\n",
    "\n",
    "# I can add Bayesian Optimization for hyperparameters, later\n",
    "class Trainer:\n",
    "    def __init__(self, excel_path, sheet_name=None):\n",
    "        self.processor = ArticleProcessor(excel_path, sheet_name)\n",
    "        self.model = None\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Process the Excel file to build the daily dataset.\n",
    "        Returns dates and aggregated daily embeddings.\n",
    "        \"\"\"\n",
    "        dates, x = self.processor.build_daily_dataset()\n",
    "        return dates, x\n",
    "    \n",
    "    def train_model(self, x, y, epochs=50):\n",
    "        \"\"\"\n",
    "        Trains the ArticleWeightPredictor using the provided data.\n",
    "        Returns the trained model.\n",
    "        \"\"\"\n",
    "        input_dim = x.shape[1]\n",
    "        predictor = ArticleWeightPredictor(input_dim)\n",
    "        predictor.train(x, y, epochs=epochs)\n",
    "        self.model = predictor\n",
    "        return predictor\n",
    "    \n",
    "    def save_model(self, model_filepath):\n",
    "        \"\"\"\n",
    "        Saves the trained model to the given filepath.\n",
    "        \"\"\"\n",
    "        if self.model:\n",
    "            self.model.save_model(model_filepath)\n",
    "        else:\n",
    "            print(\"No model has been trained yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Due to the large processing demand, all the tests were performed on the ECE Nebula server. The results are below:\n",
    "\n",
    "# Data Features\n",
    "---\n",
    "\n",
    "To establish a baseline, we first trained the model on a **single feature**, namely, the target time series itself, without incorporating any additional inputs. **Images 1 and 2** illustrate these baseline forecasts for the SP500 index, where TAF (Trend Adjusted Forecast) was applied in the first case and disabled in the second. In **Image 1**, TAF parameters were set to \\( A=0.0 \\), \\( B=0.8 \\), and \\( W=0.17 \\), resulting in a test RMSE of **101.79**. Although TAF provided some modest adjustment by slightly reducing the overall error, the model still tended to underfit abrupt market swings and displayed difficulty tracking rapid price changes over short intervals. Meanwhile, **Image 2** shows the same setup but with TAF turned off (\\( A=0, B=0, W=0 \\)); in this scenario, the RMSE climbed to **138.69**, reinforcing the notion that even a minor TAF application could improve baseline accuracy. However, neither configuration effectively captured the full volatility or nuanced price fluctuations, suggesting a clear need to **expand the feature set** and refine the trend-adjustment mechanisms. In contrast, later experiments, shown in **Images 3 through 10**, incorporated multiple features and systematically tested different TAF parameters (including cases like \\( A=0.0, B=0.0, W=0.0 \\) where TAF was calculated but provided no positive impact on RMSE, in some of the cases). As those multi-feature models typically achieved lower errors, it became evident that relying on the single feature alone was insufficient to model stock movements with the desired level of accuracy.\n",
    "\n",
    "## Using just one feature\n",
    "\n",
    "![SP500_predictions_1](images/SP500_predictions_Lr_2_H_1.0_N_100_B_256_L_5000_E_20_D_0_TAF_A_0_B_0_W_0.png)\n",
    "\n",
    "![SP500_predictions_2](images/SP500_predictions_Lr_2_H_1.0_N_100_B_256_L_5000_E_20_D_0_TAF_A_0.0_B_0.8_W_0.17.png)\n",
    "\n",
    "## Using all features\n",
    "\n",
    "![Y_AAPL](images/Y_AAPL_predictions_Lr_2_H_1.0_N_100_B_256_L_4000_E_18_D_0_TAF_A_0.1_B_0.1_W_0.19.png)\n",
    "\n",
    "## All using multi features\n",
    "\n",
    "![alt text](images/o81sckc7.png)\n",
    "\n",
    "![alt text](images/uu3pi7gu.png)\n",
    "\n",
    "![alt text](images/2kvteh0t.png)\n",
    "\n",
    "![alt text](images/owmuet3d.png)\n",
    "\n",
    "![alt text](images/v1d1p1op.png)\n",
    "\n",
    "![alt text](images/y8e910b5.png)\n",
    "\n",
    "![alt text](images/5tapcpl4.png)\n",
    "\n",
    "# Bayesian Optimization\n",
    "---\n",
    "\n",
    "To refine our model configurations and better capture the forecast horizon of **60-steps (Days)**, we employed **Bayesian Optimization** to tune key hyperparameters, particularly the **look-back window** (i.e., how many past observations the model sees at once/its most recent memory) and the number of **epochs**. In **Images 1 through 4**, we illustrate the “before” phase, where initial guesswork and manual experimentation led to suboptimal or inconsistent results. By systematically searching the hyperparameter space, Bayesian Optimization enabled us to converge on more effective settings without exhaustive trial-and-error.\n",
    "\n",
    "Once the optimal look-back and epoch combinations were identified, we explored two distinct forecasting strategies for generating the 60 future points. The first strategy was a **sequential** prediction approach, iteratively forecasting one time step at a time, moving from steps 1 to 2, then 2 to 3, and so on until reaching step 60. While straightforward, this method risks “error accumulation,” as each forecasted point influences the next prediction. The second strategy used a **TimeDistributed** layer with a **Dense** layer including all the 60 points, to predict all steps **at once**, avoiding the compounding effect of sequential errors. However, this all-at-once approach imposed significantly higher computational costs and tended to underfit given the limited training data; the model struggled to balance the complexity of producing 60 simultaneous predictions with the amount of information available, illustrated in **Imagea 1 and 2**. In **Images 3 onward**, we present the refined results after applying Bayesian Optimization, where the final model choices (including whether to use sequential or TimeDistributed forecasting) were guided by a balance of predictive accuracy, computational feasibility, and generalization potential.\n",
    "\n",
    "## Bayesian Optimization\n",
    "\n",
    "![alt text](images/67kqa5zt.png)\n",
    "\n",
    "![alt text](images/n22vhf95.png)\n",
    "\n",
    "![alt text](images/xgs782uc.png)\n",
    "\n",
    "![alt text](images/ti2o5uh4.png)\n",
    "\n",
    "## Sequential Distribution vs Bulk\n",
    "\n",
    "![alt text](images/zw8g2f1w.png)\n",
    "\n",
    "![alt text](images/f731i1gf.png)\n",
    "\n",
    "# Activation Function & Batch Size\n",
    "---\n",
    "In the next phase of experimentation, we evaluated the impact of different activation functions on our model’s predictive stability. Images 1 and 2 reveal that using ReLU led to a severe exponential decay in forecast values: after just a few time steps, the predicted series would collapse toward zero, effectively erasing any meaningful signals. To address this, we switched to tanh, which better preserved signal dynamics over the extended forecasting horizon and helped maintain more realistic output ranges. Concurrently, we tested a significantly larger batch size (1024), a move designed to accomodate more timesteps in training and inference, inadvertently dampening the beneficial effects of TAF. As seen in **Images 3 through 6**, TAF’s contribution to error reduction became negligible when the network processed such large batches, suggesting that the smoothing influence of a high batch size may already have been addressing the same temporal dynamics that TAF attempts to correct. Consequently, these findings guided us toward using moderate batch sizes and tanh activations for optimal balance between stable training behavior and effective trend adjustment.\n",
    "\n",
    "## Relu Activation Function\n",
    "\n",
    "![alt text](images/jykluk22.png)\n",
    "\n",
    "![alt text](images/7semowwj.png)\n",
    "\n",
    "## Large Batch Size\n",
    "\n",
    "![alt text](images/czka1ame.png)\n",
    "\n",
    "![alt text](images/m2nm403t.png)\n",
    "\n",
    "![alt text](images/evb2wvzf.png)\n",
    "\n",
    "![alt text](images/o2ylpjz5.png)\n",
    "\n",
    "# News Embeggings\n",
    "\n",
    "Due to lack of data, we were not able to generate the predictions based on news.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We have experimented with  a variety of techniques to reduce RMSE and improve our stock price predictions. We have been able to predict stock prices for 60 days into the future with an RMSE of 1-7.\n",
    "\n",
    "## Techniques\n",
    "\n",
    "### Best LSTM Parameters\n",
    "\n",
    "After optimizing our parameters, we found the best results using 2 layers, 100 neurons, a batch size of 256, and a lookback and number of epochs at 2786 and 348 respectively. Addtionally, we got our best results with the dropout rate set to 0. We have found that the parameters for an LSTM are all very co-dependant on eachother and are best optimized together instead of individually. Especially look back and epochs were heavily dependant on batch size.\n",
    "\n",
    "For hyperparameter optimization, we found Bayesian optimization to be very useful for training the LSTM, mostly for look back and epochs.\n",
    "\n",
    "We found results were best when all features were forcasted in the same step instead of one at a time. Conversely, we found better predictions when foreacting one time step (1 day) at a time instead of all 60 at once.\n",
    "\n",
    "### Trend Adjusted Forecast\n",
    "\n",
    "The TAF was less impactful than we hoped. For many of our trials ended with the TAF adjustments converging to 0, meaning it was not useful. When we used larger batch sizes, we found that the TAF almost never had any effects.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "We found tanh had the better performance for the stock prediction thans relu.\n",
    "\n",
    "## Insights on Data Analytics\n",
    "\n",
    "Techniques that work well for other applications did not automatically perform better in our situation (tanh vs relu). It is important to be aware of all of them and see what works in each case.\n",
    "\n",
    "## Recommendations for Future Work\n",
    "\n",
    "Most of our LSTM models suffered from underfitting, perhaps using a more complex architecture with deeper layers could address this. Also, while we could easily collect numerical data on stock, it was much more difficult to collect news data resulting in us having a very small dataset. A potential solution would be to purchase access to this news from one for the many official sources who have years of archive (including OpenBB sources)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
