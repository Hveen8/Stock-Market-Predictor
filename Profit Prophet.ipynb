{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profit Prophet: The Stock Market ML Predictor\n",
    "\n",
    "This project uses LSTM machine learning to make predictions of stock market prices.\n",
    "\n",
    "### Our Team\n",
    "\n",
    "* Daniel Oliyarnik\n",
    "* Harry van der Veen\n",
    "* Darren Sun\n",
    "* Jialu Xu\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We are trying to build a TAF enhanced LSTM to accurately predict stock market prices. We believe that stock prices can be forecasted based on their previous numerical values, but are also heavily impacted by speculation which we can capture using corprate news articles.\n",
    "\n",
    "## Stock Data\n",
    "\n",
    "We use a combination of the fundamental stock data (open price, high, low, close price) along with some calculated indicators (rsi, macd).\n",
    "\n",
    " Additionally, we use new articles related to the companies of interest.\n",
    "\n",
    "## Approach\n",
    "\n",
    "First we train an LSTM model for the fundamental data to get a forecast of the market based on pure numeric data. Then we apply a TAF to the prediction to reduce RMSE. This produces our forecast based on the numeric data.\n",
    "\n",
    "Next, we take the news data and extract embeddings to output a vector containing the relevance and estimations of stock impact. This output is combined with the fundamental stock data and pushed to another LSTM to predict future values. This provides the speculative forecast which accounts for corporate news.\n",
    "\n",
    "![Block Diagram.png](images/Block%20Diagram.png)\n",
    "\n",
    "## Tools\n",
    "\n",
    "There are multiple different ways to get the stock prices; Bloomberg Terminals, and OpenBB.\n",
    "\n",
    "News data can be imported from a variety of sources including Bloomberg and OpenBB, but also regular sites such as the Financial Times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloomberg API\n",
    "\n",
    "Bloomberg terminals are the defacto way to get stock information. UW also provides access to 4 of these terminals in the MC building. The API for Bloomberg requires the terminal to be running, so the API can only run on a machine with the terminal open.\n",
    "\n",
    "For this reason, we are moving away from Bloomberg API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloomberg API\n",
    "\n",
    "from xbbg import blp\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = './Data/'\n",
    "\n",
    "tickers = ['NVDA US Equity', 'AAPL US Equity']\n",
    "fields = ['High', 'Low', 'Last_Price']\n",
    "start_date = '2024-11-01'\n",
    "end_date = '2024-11-10'\n",
    "\n",
    "# This line hangs unless it is running with a Bloomberg terminal\n",
    "hist_tick_data = blp.bdh(tickers=tickers, fields=fields, start_date=start_date, end_date=end_date)\n",
    "\n",
    "filename = f'tick_data_{start_date}_to_{end_date}.csv'\n",
    "hist_tick_data.to_csv(DATA_DIR + filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenBB\n",
    "\n",
    "OpenBB is a free open-source implementation of Bloomberg's stock viewer. It can be run without any special software running in the background.\n",
    "\n",
    "The data is aggregated from multiple sources, though some data is inaccessible unless we purchase api keys from the corresponding sources\n",
    "\n",
    "### Finnhub\n",
    "\n",
    "Finnhub is a free data provider for corporate news. We can use this to import up to 3 months of news on a particular company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openbb\n",
    "openbb.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate limiter class\n",
    "# Some of the liraries used in the code are rate limited. This class can be used\n",
    "# to limit the number of requests made to the library in a given time period.\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class TokenBucket:\n",
    "    def __init__(self, tokens, refill_rate):\n",
    "        self.capacity = tokens  # Max tokens (60)\n",
    "        self.tokens = tokens    # Initial tokens\n",
    "        self.refill_rate = refill_rate  # Tokens added per second (60)\n",
    "        self.lock = threading.Lock()\n",
    "        self.last_refill = time.time()\n",
    "\n",
    "    def _refill(self):\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_refill\n",
    "        # Calculate tokens to add based on elapsed time\n",
    "        new_tokens = elapsed * self.refill_rate\n",
    "        if new_tokens > 0:\n",
    "            self.tokens = min(self.capacity, self.tokens + new_tokens)\n",
    "            self.last_refill = now\n",
    "\n",
    "    def consume(self, tokens=1):\n",
    "        with self.lock:\n",
    "            self._refill()\n",
    "            if self.tokens >= tokens:\n",
    "                self.tokens -= tokens\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>symbol</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>macdh</th>\n",
       "      <th>macds</th>\n",
       "      <th>headline</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>SP500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-01-02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.024994e+09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1108.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-01-02</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.309248e+09</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1108.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-01-05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.530258e+09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-01-05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.725876e+09</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-01-06</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>7.130872e+09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1123.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>2025-02-26</td>\n",
       "      <td>129.99</td>\n",
       "      <td>133.73</td>\n",
       "      <td>128.49</td>\n",
       "      <td>131.28</td>\n",
       "      <td>3.225538e+08</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>47.845685</td>\n",
       "      <td>-0.139424</td>\n",
       "      <td>-0.190138</td>\n",
       "      <td>0.050714</td>\n",
       "      <td>QTUM: Our Favorite Way To Invest In Quantum Co...</td>\n",
       "      <td>Invest in the Defiance Quantum ETF (QTUM) for ...</td>\n",
       "      <td>SeekingAlpha\\nFinnhub\\nSeekingAlpha\\nSeekingAl...</td>\n",
       "      <td>5956.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>239.66</td>\n",
       "      <td>242.46</td>\n",
       "      <td>237.06</td>\n",
       "      <td>237.30</td>\n",
       "      <td>4.115364e+07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>46.727976</td>\n",
       "      <td>2.034598</td>\n",
       "      <td>0.185772</td>\n",
       "      <td>1.848826</td>\n",
       "      <td>Auxier Asset Management Winter 2024 Market Com...</td>\n",
       "      <td>Auxier Focus Fund's Investor Class declined 2....</td>\n",
       "      <td>SeekingAlpha\\nSeekingAlpha\\nSeekingAlpha\\nMark...</td>\n",
       "      <td>5861.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>134.97</td>\n",
       "      <td>135.01</td>\n",
       "      <td>120.01</td>\n",
       "      <td>120.15</td>\n",
       "      <td>4.431758e+08</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>38.123629</td>\n",
       "      <td>-1.153285</td>\n",
       "      <td>-0.963199</td>\n",
       "      <td>-0.190086</td>\n",
       "      <td>Auxier Asset Management Winter 2024 Market Com...</td>\n",
       "      <td>Auxier Focus Fund's Investor Class declined 2....</td>\n",
       "      <td>SeekingAlpha\\nSeekingAlpha\\nSeekingAlpha\\nSeek...</td>\n",
       "      <td>5861.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>236.91</td>\n",
       "      <td>242.09</td>\n",
       "      <td>230.20</td>\n",
       "      <td>241.84</td>\n",
       "      <td>5.683336e+07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>53.196852</td>\n",
       "      <td>1.906182</td>\n",
       "      <td>0.045885</td>\n",
       "      <td>1.860297</td>\n",
       "      <td>The AI Smartphone Battle Of Titans: iPhone 16 ...</td>\n",
       "      <td>Apple's iPhone 16 and Samsung's Galaxy S25 mar...</td>\n",
       "      <td>SeekingAlpha\\nDowJones\\nMarketWatch\\nSeekingAl...</td>\n",
       "      <td>5954.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>118.02</td>\n",
       "      <td>125.09</td>\n",
       "      <td>116.40</td>\n",
       "      <td>124.92</td>\n",
       "      <td>3.890911e+08</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>43.429015</td>\n",
       "      <td>-1.553964</td>\n",
       "      <td>-1.091103</td>\n",
       "      <td>-0.462861</td>\n",
       "      <td>The Score: Home Depot, Nvidia, Tesla and More ...</td>\n",
       "      <td>The Score: Home Depot, Nvidia, Tesla and More ...</td>\n",
       "      <td>DowJones\\nSeekingAlpha\\nFinnhub\\nSeekingAlpha\\...</td>\n",
       "      <td>5954.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date    open    high     low   close        volume symbol  \\\n",
       "0     2004-01-02    0.39    0.39    0.38    0.38  2.024994e+09   AAPL   \n",
       "1     2004-01-02    0.20    0.20    0.19    0.19  1.309248e+09   NVDA   \n",
       "2     2004-01-05    0.38    0.40    0.38    0.40  5.530258e+09   AAPL   \n",
       "3     2004-01-05    0.20    0.20    0.19    0.20  1.725876e+09   NVDA   \n",
       "4     2004-01-06    0.40    0.40    0.39    0.40  7.130872e+09   AAPL   \n",
       "...          ...     ...     ...     ...     ...           ...    ...   \n",
       "10647 2025-02-26  129.99  133.73  128.49  131.28  3.225538e+08   NVDA   \n",
       "10648 2025-02-27  239.66  242.46  237.06  237.30  4.115364e+07   AAPL   \n",
       "10649 2025-02-27  134.97  135.01  120.01  120.15  4.431758e+08   NVDA   \n",
       "10650 2025-02-28  236.91  242.09  230.20  241.84  5.683336e+07   AAPL   \n",
       "10651 2025-02-28  118.02  125.09  116.40  124.92  3.890911e+08   NVDA   \n",
       "\n",
       "             rsi      macd     macdh     macds  \\\n",
       "0            NaN       NaN       NaN       NaN   \n",
       "1            NaN       NaN       NaN       NaN   \n",
       "2            NaN       NaN       NaN       NaN   \n",
       "3            NaN       NaN       NaN       NaN   \n",
       "4            NaN       NaN       NaN       NaN   \n",
       "...          ...       ...       ...       ...   \n",
       "10647  47.845685 -0.139424 -0.190138  0.050714   \n",
       "10648  46.727976  2.034598  0.185772  1.848826   \n",
       "10649  38.123629 -1.153285 -0.963199 -0.190086   \n",
       "10650  53.196852  1.906182  0.045885  1.860297   \n",
       "10651  43.429015 -1.553964 -1.091103 -0.462861   \n",
       "\n",
       "                                                headline  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "10647  QTUM: Our Favorite Way To Invest In Quantum Co...   \n",
       "10648  Auxier Asset Management Winter 2024 Market Com...   \n",
       "10649  Auxier Asset Management Winter 2024 Market Com...   \n",
       "10650  The AI Smartphone Battle Of Titans: iPhone 16 ...   \n",
       "10651  The Score: Home Depot, Nvidia, Tesla and More ...   \n",
       "\n",
       "                                                 summary  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "10647  Invest in the Defiance Quantum ETF (QTUM) for ...   \n",
       "10648  Auxier Focus Fund's Investor Class declined 2....   \n",
       "10649  Auxier Focus Fund's Investor Class declined 2....   \n",
       "10650  Apple's iPhone 16 and Samsung's Galaxy S25 mar...   \n",
       "10651  The Score: Home Depot, Nvidia, Tesla and More ...   \n",
       "\n",
       "                                                  source    SP500  \n",
       "0                                                    NaN  1108.48  \n",
       "1                                                    NaN  1108.48  \n",
       "2                                                    NaN  1122.22  \n",
       "3                                                    NaN  1122.22  \n",
       "4                                                    NaN  1123.67  \n",
       "...                                                  ...      ...  \n",
       "10647  SeekingAlpha\\nFinnhub\\nSeekingAlpha\\nSeekingAl...  5956.06  \n",
       "10648  SeekingAlpha\\nSeekingAlpha\\nSeekingAlpha\\nMark...  5861.57  \n",
       "10649  SeekingAlpha\\nSeekingAlpha\\nSeekingAlpha\\nSeek...  5861.57  \n",
       "10650  SeekingAlpha\\nDowJones\\nMarketWatch\\nSeekingAl...  5954.50  \n",
       "10651  DowJones\\nSeekingAlpha\\nFinnhub\\nSeekingAlpha\\...  5954.50  \n",
       "\n",
       "[10652 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OpenBB API\n",
    "\n",
    "from openbb import obb\n",
    "import finnhub\n",
    "from ftfy import fix_text\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor  # Use ProcessPoolExecutor for CPU-bound tasks\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=\"[Key removed - please add your own]\")\n",
    "obb.user.preferences.output_type = 'dataframe'\n",
    "rate_limiter = TokenBucket(tokens=60, refill_rate=60)\n",
    "\n",
    "FAST = 12\n",
    "SLOW = 26\n",
    "SIGNAL = 9\n",
    "\n",
    "MIN_POINTS = SLOW + SIGNAL - 1\n",
    "DAYS_TO_PAD = -(-(MIN_POINTS * 1.5) // 1) # Not every day has data. Round up to nearest integer\n",
    "\n",
    "def process_symbol_data(symbol, start_date, end_date):\n",
    "    # Wait until a token is available\n",
    "    while not rate_limiter.consume():\n",
    "        time.sleep(0.001)  # Avoid busy-waiting\n",
    "        \n",
    "    # Process data for a single symbol with technical indicators\n",
    "    try:\n",
    "        # Fetch OHLCV data\n",
    "        symbol_data_df = obb.equity.price.historical(symbol, start_date=start_date, end_date=end_date)\n",
    "        symbol_data_df['symbol'] = symbol  # Add symbol column\n",
    "\n",
    "        # Remove any duplicate dates\n",
    "        symbol_data_df = symbol_data_df[~symbol_data_df.index.duplicated(keep='first')] \n",
    "\n",
    "        # RSI\n",
    "        symbol_data_df = obb.technical.rsi(data=symbol_data_df, target='close', length=14, scalar=100.0, drift=1)\n",
    "        symbol_data_df.rename(columns={'close_RSI_14': 'rsi'}, inplace=True)\n",
    "\n",
    "        # MACD\n",
    "        symbol_data_df = obb.technical.macd(data=symbol_data_df, target='close', fast=FAST, slow=SLOW, signal=SIGNAL)\n",
    "        symbol_data_df.rename(columns={f'close_MACD_{str(FAST)}_{str(SLOW)}_{str(SIGNAL)}': 'macd',\n",
    "                                       f'close_MACDh_{str(FAST)}_{str(SLOW)}_{str(SIGNAL)}': 'macdh',\n",
    "                                       f'close_MACDs_{str(FAST)}_{str(SLOW)}_{str(SIGNAL)}': 'macds'}, inplace=True)\n",
    "        \n",
    "        # Convert 'date' index to regular index\n",
    "        symbol_data_df.reset_index(inplace=True)\n",
    "\n",
    "        # News\n",
    "        symbol_news = finnhub_client.company_news(symbol, _from=start_date, to=end_date)\n",
    "\n",
    "        # Fix encoding for all text fields in the raw API response\n",
    "        for article in symbol_news:\n",
    "            for text_field in ['headline', 'summary', 'source']:\n",
    "                if text_field in article and article[text_field] is not None:\n",
    "                    article[text_field] = fix_text(article[text_field])\n",
    "\n",
    "        group_column = 'date'\n",
    "        text_columns = ['headline', 'summary', 'source']\n",
    "\n",
    "        symbol_news_df = (pd.DataFrame(symbol_news)\n",
    "            .assign(datetime=lambda x: pd.to_datetime(x['datetime'], unit='s', errors=\"coerce\"))\n",
    "            .dropna(subset=[\"datetime\"])  # Remove invalid rows\n",
    "            .assign(datetime=lambda x: x[\"datetime\"].dt.strftime(\"%Y-%m-%d\"))\n",
    "            .rename(columns={'datetime': group_column})\n",
    "            [[group_column] + text_columns]\n",
    "        )\n",
    "\n",
    "        # Ensure 'date' is datetime in both DataFrames\n",
    "        symbol_data_df['date'] = pd.to_datetime(symbol_data_df['date'])\n",
    "        symbol_news_df['date'] = pd.to_datetime(symbol_news_df['date'])\n",
    "\n",
    "        # Aggregate news data to one row per date\n",
    "        symbol_news_df = symbol_news_df.groupby('date').agg({\n",
    "            'headline': lambda x: '\\n'.join(x.astype(str)),\n",
    "            'summary': lambda x: '\\n'.join(x.astype(str)),\n",
    "            'source': lambda x: '\\n'.join(x.astype(str))\n",
    "        }).reset_index()\n",
    "\n",
    "        symbol_data_df = symbol_data_df.merge(symbol_news_df, on='date', how='outer')\n",
    "\n",
    "        return symbol_data_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {symbol}: {traceback.format_exc()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def downloadStockData(symbols, start_date=None, end_date=None, parallel=True):\n",
    "    try:\n",
    "        # Fetch S&P 500 data once for all symbols\n",
    "        sp500_df = obb.equity.price.historical(\"SPX\", start_date=start_date, end_date=end_date)\n",
    "        sp500_df = sp500_df[['close']].rename(columns={'close': 'SP500'})\n",
    "        sp500_df.reset_index(inplace=True)\n",
    "        sp500_df['date'] = pd.to_datetime(sp500_df['date'])\n",
    "\n",
    "        # Process symbols in parallel or sequentially\n",
    "        if parallel:\n",
    "            with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                futures = [executor.submit(process_symbol_data, symbol, start_date, end_date) for symbol in symbols]\n",
    "                results = [f.result() for f in futures]\n",
    "        else:\n",
    "            results = [process_symbol_data(symbol, start_date, end_date) for symbol in symbols]\n",
    "\n",
    "        # Combine all symbols and merge with SP500\n",
    "        combined_df = pd.concat(results)\n",
    "\n",
    "        final_df = combined_df.merge(sp500_df, on='date', how='outer')        \n",
    "\n",
    "        # Add groupby-aware technical calculations\n",
    "        final_df = final_df.groupby('symbol', group_keys=False).apply(lambda x: x.sort_index())\n",
    "        final_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "        return final_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {traceback.format_exc()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Declare search bounds \n",
    "symbols = ['AAPL', 'NVDA']\n",
    "start_date = '1950-01-01'\n",
    "end_date = '2025-03-01'\n",
    "\n",
    "data_df = downloadStockData(symbols, start_date, end_date)\n",
    "\n",
    "data_df.to_csv('Stock Data.csv')\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessor\n",
    "\n",
    "We create an extended MinMaxScaler which includes extra headroom for future values. This class is also used to create a sliding window dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class BufferedMinMaxScaler(MinMaxScaler):\n",
    "    \"\"\"New Subclass, inheriting from MinMaxScalar, gives custom/buffered scale\"\"\"\n",
    "    def __init__(self, headroom=0.2):\n",
    "        super().__init__()\n",
    "        self.headroom = headroom\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X)\n",
    "        # 1. Store original data min/max\n",
    "        self.orig_data_min_ = X.min(axis=0)\n",
    "        self.orig_data_max_ = X.max(axis=0)\n",
    "        #self.data_min_ = X.min(axis=0)\n",
    "        #self.data_max_ = X.max(axis=0)\n",
    "\n",
    "        # 2. Calculate buffer-adjusted max\n",
    "        data_range = self.orig_data_max_ - self.orig_data_min_\n",
    "        self.data_max_ = self.orig_data_max_ + data_range * self.headroom\n",
    "        self.data_min_ = self.orig_data_min_  # Keep original min (for now, potentially will need to change)\n",
    "\n",
    "        # 3. Calculate parent class parameters (data_range_ is for parent class, incorporating the headroom)\n",
    "        self.data_range_ = self.data_max_ - self.data_min_\n",
    "        self.scale_ = (self.feature_range[1] - self.feature_range[0]) / self.data_range_\n",
    "        self.min_ = self.feature_range[0] - self.data_min_ * self.scale_\n",
    "        return self \n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, headroom=0.2):\n",
    "        # Its using the extra headroom by defult\n",
    "        self.scaler = BufferedMinMaxScaler(headroom=headroom)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.scaler.fit_transform(data)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "    def create_dataset(self, dataset, look_back=1, target_feature=0, forecast_horizon=1):\n",
    "        dataX, dataY = [], []\n",
    "\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            # Note the : here indicated to put into a 2D array [[1,2,3], [1,2,3], [1,2,3]]\n",
    "            input = dataset[i:(i+look_back), :] # IF there was only 1 feature, then need 0 to put into 1D\n",
    "            # Appened into shape(X, 3)\n",
    "            dataX.append(input)\n",
    "            output = dataset[i + look_back, :]\n",
    "            # output = dataset[i+look_back:i+look_back+forecast_horizon, :]\n",
    "            dataY.append(output)\n",
    "\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    def trim_XY(self, dataX, dataY, batch_size):\n",
    "        # Removing any odd data depending on batch size\n",
    "        trim_size = len(dataX) - (len(dataX) % batch_size)\n",
    "        return dataX[:trim_size], dataY[:trim_size]\n",
    "\n",
    "    def invert_1d_prediction(self, pred_1d, feature_cols_num, target_feature=0):\n",
    "        # If we are looking at ALL the included features\n",
    "        if pred_1d.shape[1] == feature_cols_num:\n",
    "            inverted = self.scaler.inverse_transform(pred_1d)\n",
    "        else:\n",
    "            # pred_1d shape: (samples,1) -> what the INFERENCE output of the LSTM is\n",
    "            # (samples, num_features) -> (samples,1) keeping target column\n",
    "            padded = np.zeros((pred_1d.shape[0], feature_cols_num), dtype=np.float32)\n",
    "            # [0, 0, 1, 0]\n",
    "            # [0, 0, 1, 0] <- is essentially this\n",
    "            # [0, 0, 1, 0]\n",
    "            if pred_1d.shape[1] == 1:\n",
    "                padded[:, target_feature] = pred_1d[:, 0]\n",
    "            else:\n",
    "                padded[:, target_feature] = pred_1d[:, target_feature]\n",
    "            inverted = self.scaler.inverse_transform(padded)\n",
    "        # Return just the \"target\" column (out of all the feature columns)\n",
    "        return inverted[:, target_feature].reshape(-1,1)\n",
    "\n",
    "def filter_multi_features(dataset, stock_rows, feature_cols):\n",
    "    df_symbol = dataset[dataset['symbol'] == stock_rows].copy()\n",
    "    df_symbol[feature_cols] = df_symbol[feature_cols].fillna(0)\n",
    "    full_dataset = df_symbol[feature_cols].values.astype('float32')\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "\n",
    "The LSTM is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, InputLayer, RepeatVector, TimeDistributed\n",
    "\n",
    "class LSTMModel:\n",
    "    def __init__(self, layers, look_back, batch_size, neurons, epochs, activation, dropout, features=1, isReturnSeq=False, forecast_horizon=1):\n",
    "        self.layers = layers\n",
    "        self.isReturnSeq = isReturnSeq # should be either True or False\n",
    "        self.features = features\n",
    "        self.look_back = look_back\n",
    "        self.batch_size = batch_size\n",
    "        self.neurons = neurons\n",
    "        self.epochs = epochs\n",
    "        self.activation = activation # should be either the str 'tanh' or 'relu'\n",
    "        self.dropout = dropout\n",
    "        self.model = self._build_model(self.batch_size, forecast_horizon) \n",
    "        # What the class will fill\n",
    "        self.trainPredict = None\n",
    "\n",
    "    def _build_model(self, batch_size, forecast_horizon):\n",
    "        # SHOULD ONLY HAVE ONE MODEL IN MEMORY (TF handles the models in memory in a funny (funny = i dont know))\n",
    "        # So clear_session is to clear the way tf stores/handles the models\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = Sequential()\n",
    "        # batch_input_shape (batch_size, num_steps, features)\n",
    "        model.add(InputLayer(batch_input_shape=(batch_size, self.look_back, self.features)))\n",
    "        # the more complex the data -> more neurons needed\n",
    "        if self.layers > 1:\n",
    "            for l in range(self.layers-1):\n",
    "                model.add(LSTM(self.neurons, activation=self.activation, dropout=self.dropout, stateful=True, return_sequences=True))\n",
    "            # In multi-layered the last is non-stateful ***\n",
    "            model.add(LSTM(self.neurons, activation=self.activation, dropout=self.dropout, return_sequences=self.isReturnSeq))\n",
    "        else:\n",
    "            model.add(LSTM(self.neurons, activation=self.activation, dropout=self.dropout, stateful=True, return_sequences=self.isReturnSeq))\n",
    "        model.add(Dense(9))\n",
    "        # # Repeat the final hidden state to produce forecast_horizon steps (Our forecast)\n",
    "        # model.add(RepeatVector(forecast_horizon))\n",
    "        # # Use an LSTM that returns sequences, or simply a TimeDistributed Dense layer\n",
    "        # model.add(LSTM(self.neurons, activation=self.activation, dropout=self.dropout, return_sequences=True))\n",
    "        # model.add(TimeDistributed(Dense(self.features)))  # Now outputs shape (batch_size, forecast_horizon, features)\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    # def create_input_layer(self, batch_size, look_back):\n",
    "    #     # store the generated input layer into the class\n",
    "    #     self.input_layer = InputLayer(batch_input_shape=(batch_size, look_back, 1))\n",
    "    #     return self.input_layer\n",
    "\n",
    "    def reset_model_states(self):\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, LSTM) and layer.stateful:\n",
    "                layer.reset_states()\n",
    "        print('Model states reset')\n",
    "\n",
    "    def train(self, trainX, trainY):\n",
    "        self.reset_model_states()\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # Unlike CNNs, for RNNs (LSTM fitted) we do not shuffle\n",
    "            self.model.fit(trainX, trainY, \n",
    "                        epochs=1, \n",
    "                        batch_size=self.batch_size, \n",
    "                        shuffle=False, \n",
    "                        verbose=2)\n",
    "            \n",
    "            # Resetting states after each epoch for stateful LSTM\n",
    "            self.reset_model_states()\n",
    "            print(f\"Epoch {i+1}/{self.epochs} --- Completed\")\n",
    "\n",
    "    def predict(self, trainX):\n",
    "        # X is the training data\n",
    "        self.trainPredict = self.model.predict(trainX, batch_size=self.batch_size)\n",
    "        return self.trainPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Engine\n",
    "\n",
    "The Forecast Engine allows us to create longer predictions by feeding the last prediction into the input of the next one, in a rolling window fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class ForecastEngine(LSTMModel):\n",
    "    def __init__(self, trained_model, layers=None, isReturnSeq=True, features=None, look_back=None, batch_size=None, neurons=None, epochs=None, activation=None, dropout=None):\n",
    "        params = {'layers': layers,\n",
    "                'isReturnSeq' : isReturnSeq,\n",
    "                'features' : features,\n",
    "                'look_back': look_back,\n",
    "                'batch_size': batch_size,\n",
    "                'neurons': neurons,\n",
    "                'epochs': epochs,\n",
    "                'activation': activation,\n",
    "                'dropout': dropout}\n",
    "\n",
    "        # Use parameters from trained_model or default values IF specified\n",
    "        for param_name in params:\n",
    "            if params[param_name] is None:\n",
    "                params[param_name] = getattr(trained_model, param_name)\n",
    "\n",
    "        # Initialize LSTMModel with inherited parameters\n",
    "        super().__init__(layers=params['layers'],\n",
    "                         isReturnSeq=params['isReturnSeq'],\n",
    "                         features=params['features'],\n",
    "                         look_back=params['look_back'],\n",
    "                         batch_size=params['batch_size'],\n",
    "                         neurons=params['neurons'],\n",
    "                         epochs=params['epochs'],\n",
    "                         activation=params['activation'],\n",
    "                         dropout=params['dropout'])\n",
    "\n",
    "        self.trained_model = trained_model\n",
    "        # What the class will fill\n",
    "        self.futurePredictions = None\n",
    "\n",
    "    def forecast(self, start_input, steps, target_col_idx=0):\n",
    "        # Use _build_model from LSTMModel to create a new model for forecasting\n",
    "        forecast_model = self._build_model(self.batch_size, forecast_horizon=steps)  # Rebuild model for forecasting\n",
    "\n",
    "        # Set weights from the trained model\n",
    "        forecast_model.set_weights(self.trained_model.model.get_weights())\n",
    "\n",
    "        new_predictions = []\n",
    "        current_batch = start_input[-self.batch_size:]  # Get the last batch for prediction\n",
    "        # current_batch = [current_batch.copy() for _ in range(self.batch_size)]\n",
    "        # last_look_back_seq = start_input[-1]\n",
    "\n",
    "        # print('Future batch steps: ', math.ceil( steps/self.batch_size ))\n",
    "        # for i in range(math.ceil( steps/self.batch_size )):\n",
    "        # # ===================================================================\n",
    "        for i in range(steps):\n",
    "        #     # # (1, look_back, num_features)\n",
    "        #     # current_input = np.expand_dims(last_look_back_seq, axis=0)\n",
    "        #     # # Can do this (batch_size=1) since have Dense(1)\n",
    "        #     # pred = forecast_model.predict(current_input, batch_size=1)\n",
    "        #     # next_target_value = pred[0, 0]\n",
    "            \n",
    "        #     # new_predictions.append(next_target_value)\n",
    "\n",
    "\n",
    "        #     # new_step = np.copy(last_look_back_seq[-1])\n",
    "        #     # # Replace the target column with the prediction. Other features stay the same\n",
    "        #     # new_step[target_col_idx] = next_target_value\n",
    "\n",
    "\n",
    "        #     # last_look_back_seq = np.vstack((last_look_back_seq[1:], new_step))\n",
    "\n",
    "\n",
    "            pred = forecast_model.predict(current_batch, batch_size=self.batch_size)\n",
    "\n",
    "            new_predictions.append(pred[-1, -1, target_col_idx])\n",
    "\n",
    "        #     # Get all the next infered values/timesteps, put into 3D shape\n",
    "            new_step = pred[:, -1, :].reshape(self.batch_size, 1, -1)\n",
    "            # new_step = pred[:, -1:]\n",
    "\n",
    "        #     # for b in range(self.batch_size):\n",
    "        #     #     new_predictions.append(pred[b, -1, 0])\n",
    "        #     # print('Predictions shape: ', pred[i, -1, :].shape)\n",
    "\t\t#     # print('New Inference (Prediction): ', pred[-1, -1, 0])\n",
    "\n",
    "        #     # # Update each sequence in the batch\n",
    "        #     # new_batch = np.zeros_like(current_batch)\n",
    "        #     # for b in range(self.batch_size):\n",
    "        #     #     # Shift the sequence\n",
    "        #     #     rolled = np.roll(current_batch[b], -1)\n",
    "        #     #     # Add the new prediction at the end\n",
    "        #     #     rolled[-1] = pred[b, -1, 0]\n",
    "        #     #     new_batch[b] = rolled\n",
    "        #     # current_batch = new_batch\n",
    "\n",
    "        #     # Update of batch for next prediction step, dropping the oldest value (in look back) and\n",
    "        #     # adding the new infered values (newest in look back)\n",
    "            current_batch = np.concatenate([current_batch[:, 1:, :], new_step], axis=1)\n",
    "        # ===================================================================\n",
    "\n",
    "        # # Multi-Step Forecasting (Takes advantage of the TimeDistributed Dense layer)\n",
    "        # multi_step_forecast = forecast_model.predict(current_batch, batch_size=self.batch_size)\n",
    "        # print(\"multi_step_forecast shape:\", multi_step_forecast.shape)\n",
    "        # new_predictions = multi_step_forecast[0]\n",
    "        # # new_predictions = multi_step_forecast[-1, :, target_col_idx]\n",
    "        # # new_predictions = np.mean(multi_step_forecast[:, :, target_col_idx], axis=0)\n",
    "        # print(\"new_predictions shape:\", new_predictions.shape)\n",
    "\n",
    "        # Convert predictions to a numpy array (predictions_array)\n",
    "        self.futurePredictions = np.array(new_predictions).reshape(-1, 1)\n",
    "        # self.futurePredictions = new_predictions\n",
    "\n",
    "        # ***** May need to manage resetting the states better, however may not be necessary \n",
    "\n",
    "        return self.futurePredictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAF Shift\n",
    "\n",
    "To apply the TAF, we calculate the smoothed error and trend factors to adjust predictions, with a grid search function to find optimal alpha, beta, and weight parameters that minimize prediction error against test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TAFShift:\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        self.alpha = alpha  # Smoothing constant for error TAF\n",
    "        self.beta = beta    # Smoothing constant for trend in TAF\n",
    "\n",
    "    def calculate_taf(self, data, predictions):\n",
    "        \"\"\"full_taf, predicted_taf\"\"\"\n",
    "        data = np.asarray(data).flatten()\n",
    "        predictions = np.asarray(predictions).flatten()\n",
    "\n",
    "        n = len(data) + len(predictions)\n",
    "\n",
    "        # Create a single array with all datapoints\n",
    "        combined_data = np.concatenate([data, predictions])\n",
    "        # combined_data = np.vstack((historical_data, predictions_array))\n",
    "        total_length = len(combined_data)\n",
    "\n",
    "        st = np.zeros(total_length) # Smooth error\n",
    "        tt = np.zeros(total_length) # Trend factor\n",
    "        taf_values = np.zeros(total_length)\n",
    "\n",
    "        # Using the first timestep (from data) as initial smoothed forecast\n",
    "        st[0] = combined_data[0]\n",
    "        tt[0] = combined_data[1] - combined_data[0]\n",
    "\n",
    "        taf_values = np.zeros(n)\n",
    "        \n",
    "        # Calculate TAF values for the complete dataset:\n",
    "        # https://courses.worldcampus.psu.edu/welcome/mangt515/lesson02_13.html\n",
    "        for t in range(1, n):\n",
    "            taf_values[t] = st[t-1] + tt[t-1]\n",
    "            st[t] = taf_values[t] + self.alpha*(combined_data[t] - taf_values[t])\n",
    "            tt[t] = tt[t-1] + self.beta*(taf_values[t] - taf_values[t-1] - tt[t-1])\n",
    "\n",
    "        \n",
    "        # return taf_values.reshape(-1, 1), taf_values[-len(predictions_array):].reshape(-1, 1)\n",
    "        # print(taf_values[len(data):])\n",
    "        return taf_values, taf_values[len(data):]\n",
    "        # return taf_values.reshape(-1, 1), taf_values[len(data):].reshape(-1, 1)\n",
    "\n",
    "    def apply_taf(self, historical_data, forecasted, normalize=False, weight=0.0):\n",
    "        _, predicted_taf = self.calculate_taf(historical_data, forecasted)\n",
    "        # We need to reshape due to applying flatten in calculate_taf must be in (n, 1) shape for plotting\n",
    "        predicted_taf = predicted_taf.reshape(-1, 1)\n",
    "        # predicted_taf = predicted_taf\n",
    "\n",
    "        # using Robust scaling\n",
    "        if normalize:\n",
    "            median_taf = np.median(predicted_taf)\n",
    "            q1, q3 = np.percentile(predicted_taf, [22, 95])\n",
    "            iqr = q3 - q1\n",
    "            if iqr < 1e-6:  \n",
    "                iqr = np.std(predicted_taf) if np.std(predicted_taf) > 0 else 1.0\n",
    "            predicted_taf = (predicted_taf - median_taf) / iqr \n",
    "            # predicted_taf *= weight \n",
    "\n",
    "        # adjusted_forecast = forecasted * (1 + weight * predicted_taf)\n",
    "        adjusted_forecast = forecasted + weight * predicted_taf\n",
    "        return adjusted_forecast\n",
    "    \n",
    "\n",
    "def taf_search_test(calculate_rmse, historical_data, forecasted, test_data, normalize=False):\n",
    "    \"\"\"Function for getting optimal TAF\"\"\"\n",
    "    optimal_TAF_params = {}\n",
    "    alpha_range = np.arange(0.0, 1.0, 0.1)\n",
    "    optimal_alpha = 0\n",
    "    beta_range = np.arange(0.0, 1.0, 0.1)\n",
    "    optimal_beta = 0\n",
    "    weight_range = np.arange(0.0, 0.2, 0.01)\n",
    "    optimal_weight = 0\n",
    "\n",
    "    # Relys on the assumption that the weights and TAF parameters (alpha and beta) affect RMSE independently\n",
    "    lowest_rmse = 1000000\n",
    "    optimal_taf = TAFShift()\n",
    "    optimalTAF_forecast = None\n",
    "    for a in alpha_range:\n",
    "        for b in beta_range:\n",
    "            taf_shift = TAFShift(alpha=a, beta=b)\n",
    "            # adjusted_forecast = taf_shift.apply_taf(historical_data, forecasted, normalize, weight=0.5)\n",
    "            for w in weight_range:\n",
    "                adjusted_forecast = taf_shift.apply_taf(historical_data, forecasted, normalize, weight=w)\n",
    "                rmse = calculate_rmse(adjusted_forecast[:, 0], test_data)\n",
    "                if rmse < lowest_rmse:\n",
    "                    lowest_rmse = rmse\n",
    "                    optimal_alpha = a\n",
    "                    optimal_beta = b\n",
    "                    optimal_weight = w\n",
    "                    optimalTAF_forecast = adjusted_forecast\n",
    "    # for a in alpha_range:\n",
    "    #     for b in beta_range:\n",
    "    #         taf_shift = TAFShift(alpha=a, beta=b)\n",
    "    #         adjusted_forecast = taf_shift.apply_taf(historical_data, forecasted, normalize, weight=1.0)\n",
    "    #         rmse = calculate_rmse(adjusted_forecast[:, 0], test_data[:, 0])\n",
    "    #         if rmse < lowest_rmse:\n",
    "    #             optimal_alpha = a\n",
    "    #             optimal_beta = b\n",
    "    #             optimal_taf = taf_shift         \n",
    "    #     for w in weight_range:\n",
    "    #         # adjusted_forecast = optimal_taf.apply_taf(historical_data, forecasted, normalize, weight=w)\n",
    "    #         rmse = calculate_rmse(adjusted_forecast[:, 0], test_data[:, 0])\n",
    "    #         if rmse < lowest_rmse:\n",
    "    #             lowest_rmse = rmse\n",
    "    #             optimal_weight = w\n",
    "    #             optimalTAF_forecast = adjusted_forecast\n",
    "\n",
    "    print(f\"TAF alpha: {optimal_alpha}, beta: {optimal_beta}, weight: {optimal_weight} | RMSE={lowest_rmse:.2f}\")\n",
    "\n",
    "    optimal_TAF_params[(optimal_alpha, optimal_beta, optimal_weight)] = (lowest_rmse, optimalTAF_forecast)\n",
    "    return optimal_TAF_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "This code implements time series cross-validation for forecasting models with a focus on LSTM networks. It preprocesses data, trains models, generates forecasts, and evaluates predictions using RMSE metrics, with optional TAF optimization to improve forecast accuracy through parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calculate_rmse(true_values, predicted_values):\n",
    "    return np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "\n",
    "def time_series_cross_validation(curr_dataset, model_params, forecast_horizon, initial_train_size, step_size, target_feature_col=0, OPTIMAL=False):\n",
    "# def time_series_cross_validation(curr_dataset, model_params, forecast_horizon, initial_train_size, step_size):\n",
    "    \"\"\"\n",
    "    curr_dataset: the full dataset (2D array, e.g. shape (n_samples, 1))\n",
    "    model_params: dict with keys: look_back, batch_size, epochs, headroom, dropout, etc.\n",
    "    forecast_horizon: number of points to forecast in each fold\n",
    "    initial_train_size: the initial number of samples used for training\n",
    "    step_size: number of samples to roll forward between folds\n",
    "    taf_params_list: list of tuples (alpha, beta, weight) to test\n",
    "\n",
    "    Returns: list of RMSE values, one per fold\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    curr_dataset = np.nan_to_num(curr_dataset, nan=0.0)\n",
    "    rmse_list = []\n",
    "    n = len(curr_dataset)\n",
    "    start = 0\n",
    "    # for start in range(0, n - initial_train_size - forecast_horizon + 1, step_size): \n",
    "    train_data = curr_dataset[start:start+initial_train_size]\n",
    "\n",
    "    data_preprocessor = DataPreprocessor(headroom=model_params['headroom'])\n",
    "    scaled_train = data_preprocessor.fit_transform(train_data)\n",
    "\n",
    "    \n",
    "\n",
    "    dataX, dataY = data_preprocessor.create_dataset(scaled_train, \n",
    "                                                    look_back=model_params['look_back'],\n",
    "                                                    target_feature=target_feature_col)\n",
    "    if len(dataX) == 0:\n",
    "        raise ValueError(\"No training/data samples generated (dataX). Look back may be too big\")\n",
    "    dataX, dataY = data_preprocessor.trim_XY(dataX, dataY, model_params['batch_size'])\n",
    "\n",
    "    effective_train_samples = len(dataX)\n",
    "    effective_train_end = start + effective_train_samples + model_params['look_back']\n",
    "\n",
    "    # test_end = effective_train_end + math.ceil(forecast_horizon/model_params['batch_size'])*model_params['batch_size']\n",
    "    test_end = effective_train_end + forecast_horizon\n",
    "    test_data = curr_dataset[effective_train_end:test_end]\n",
    "    print('test_data shape: ', test_data.shape)\n",
    "\n",
    "    print('*********** Starting New Cross-Validation ***********')\n",
    "    print(f\"Fold (Cross Validation) with train indices {start}:{effective_train_end} and test indices {effective_train_end}:{test_end}\")\n",
    "\n",
    "# THIS IS WHAT I AM CHANGING FOR THE MULTI FEATURES\n",
    "    # *****Need to tie input layer to Model class*****\n",
    "    # trainX = np.reshape(dataX, (dataX.shape[0], dataX.shape[1], 1))\n",
    "    # NO NEED to reshape since dataX is already 3D, (samples, look_back, num_features)\n",
    "    trainX = dataX\n",
    "    print('trainX shape (After Reshape): ', trainX.shape)\n",
    "    trainY = dataY\n",
    "\n",
    "    lstm_model = LSTMModel(layers=model_params['layers'],\n",
    "                            isReturnSeq=False,\n",
    "                            features=model_params['features'],\n",
    "                            look_back=model_params['look_back'],\n",
    "                            batch_size=model_params['batch_size'],\n",
    "                            neurons=model_params['neurons'],\n",
    "                            epochs=model_params['epochs'],\n",
    "                            activation=model_params['activation'],\n",
    "                            dropout=model_params['dropout'],\n",
    "                            forecast_horizon=forecast_horizon)\n",
    "    lstm_model.train(trainX, trainY)\n",
    "    trainPredict = lstm_model.predict(trainX)\n",
    "\n",
    "    forecast_engine = ForecastEngine(trained_model=lstm_model, isReturnSeq=True)\n",
    "    forecastPredict = forecast_engine.forecast(trainX, forecast_horizon, target_feature_col)\n",
    "    print('Forecast infered data shape: ', forecastPredict.shape)\n",
    "    print(\"NaNs in forecastPredict:\", np.isnan(forecastPredict).sum())\n",
    "\n",
    "    # Invert the scaling for the forecast, train and test data\n",
    "    forecasted_inverted = data_preprocessor.invert_1d_prediction(forecastPredict, model_params['features'], target_feature_col)\n",
    "    print(\"NaNs in forecasted_inverted:\", np.isnan(forecasted_inverted).sum())\n",
    "\n",
    "    # Ensuring forecasted_inverted has the same number of rows as test_data\n",
    "    # I ran into issues with mismatch due to th batch size\n",
    "    num_test_samples = test_data.shape[0]\n",
    "    if forecasted_inverted.shape[0] > num_test_samples:\n",
    "        forecasted_inverted = forecasted_inverted[:num_test_samples]\n",
    "    elif forecasted_inverted.shape[0] < num_test_samples:\n",
    "        test_data = test_data[:forecasted_inverted.shape[0]]\n",
    "    # train_predict_inverted = data_preprocessor.invert_1d_prediction(trainPredict, model_params['features'], target_feature_col)\n",
    "    train_predict_inverted = 0\n",
    "    # test_data_inverted  = data_preprocessor.scaler.inverse_transform(test_data)\n",
    "\n",
    "    # Per params, generating different TAF shifts\n",
    "    # rmse_results = {}\n",
    "    # for (alpha, beta, weight) in taf_params_list:\n",
    "    #     taf_shift = TAFShift(alpha=alpha, beta=beta)\n",
    "    #     # Here, we use the (inverted) training predictions as the historical base.\n",
    "    #     # You might choose a different historical reference (e.g., the raw train data)\n",
    "    #     adjusted_forecast = taf_shift.apply_taf(scaled_train, forecasted_inverted, weight=weight, normalize=False)\n",
    "    #     rmse_taf = calculate_rmse(adjusted_forecast[:, 0], test_data[:, 0])\n",
    "    #     rmse_results[(alpha, beta, weight)] = (rmse_taf, adjusted_forecast)\n",
    "    #     print(f\"TAF alpha={alpha}, beta={beta}, weight={weight}: RMSE={rmse_taf:.2f}\")\n",
    "    rmse_taf_preTAF = calculate_rmse(forecasted_inverted[:, 0], test_data[:, target_feature_col])\n",
    "    print('pre TAF: ', rmse_taf_preTAF)\n",
    "    # rmse_results = taf_search_test(calculate_rmse, scaled_train, forecasted_inverted, test_data, normalize=False) \n",
    "    if OPTIMAL:\n",
    "        rmse_TAF_results = taf_search_test(calculate_rmse, scaled_train[:effective_train_end, target_feature_col], forecasted_inverted, test_data[:, target_feature_col], normalize=False) \n",
    "        return [data_preprocessor, lstm_model, forecast_engine], train_predict_inverted, effective_train_end, test_end, forecasted_inverted, rmse_taf_preTAF, rmse_TAF_results\n",
    "    else:\n",
    "        return [data_preprocessor, lstm_model, forecast_engine], train_predict_inverted, effective_train_end, test_end, forecasted_inverted, rmse_taf_preTAF\n",
    "\n",
    "\n",
    "    # Calculate RMSE between forecast and actual test data\n",
    "    # rmse = np.sqrt(np.mean((forecasted_inverted[:, 0] - test_data_inverted[:, 0]) ** 2))\n",
    "\n",
    "    # rmse = calculate_rmse(forecasted_inverted[:, 0], test_data[:, 0])\n",
    "    # print(f\"Fold RMSE: {rmse:.2f}\")\n",
    "    # rmse_list.append(rmse)\n",
    "\n",
    "\n",
    "    # return [data_preprocessor, lstm_model, forecast_engine], train_data_inverted, effective_train_end, test_end, forecasted_inverted, rmse_list\n",
    "    # return [data_preprocessor, lstm_model, forecast_engine], train_predict_inverted, effective_train_end, test_end, rmse_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer\n",
    "\n",
    "This code plots the forecasted predictions against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class Visualizer:\n",
    "    def __init__(self, scaler: 'BufferedMinMaxScaler', trained_model: 'LSTMModel', forecast_engine: 'ForecastEngine'):\n",
    "        \"\"\"Instance needs to be of || BufferedMinMaxScaler | LSTMModel | ForecastEngine || type\"\"\"\n",
    "        if not isinstance(scaler, BufferedMinMaxScaler):\n",
    "            raise TypeError(\"Expected an instance of BufferedMinMaxScaler.\")\n",
    "        if not isinstance(trained_model, LSTMModel):\n",
    "            raise TypeError(\"Expected an instance of LSTMModel.\")\n",
    "        if not isinstance(forecast_engine, ForecastEngine):\n",
    "            raise TypeError(\"Expected an instance of ForecastEngine.\")\n",
    "        self.scaler = scaler\n",
    "        self.trained_model = trained_model\n",
    "        self.forecast_engine = forecast_engine\n",
    "\n",
    "    # def invert_predictions(self, predictions):\n",
    "    #     return self.scaler.inverse_transform(predictions)\n",
    "\n",
    "    # def calculate_rmse(self, true_values, predicted_values):\n",
    "    #     return np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "\n",
    "    def create_plot_array(self, full_length, loaction_num, value_arr):\n",
    "        # full_length = len(historical) + len(forecasts)\n",
    "        plot_array = np.zeros((len(full_length), 1))\n",
    "        plot_array[loaction_num:loaction_num+len(value_arr)] = value_arr\n",
    "        return plot_array\n",
    "\n",
    "    # def plot_results(self, rmse, train_predictions_inverted, train_end, test_end, future_predictions_inverted, curr_dataset, curr_system, curr_dir):\n",
    "    def plot_results(self, rmse, train_predictions_inverted, train_end, test_end, future_predictions_inverted, curr_dataset, curr_system, target_col, curr_dir, TAFvars):\n",
    "        \"\"\"Both Train and Forecast must be given INVERTED, following the .predict/forecast output\"\"\"\n",
    "        # Extract parameters from the trained model\n",
    "        look_back = self.trained_model.look_back\n",
    "        batch_size = self.trained_model.batch_size\n",
    "        neurons = self.trained_model.neurons\n",
    "        epochs = self.trained_model.epochs\n",
    "        dropout = self.trained_model.dropout\n",
    "        train_predictions = self.trained_model.trainPredict\n",
    "\n",
    "        # Extract parameters from the forecasted data\n",
    "        layers = self.forecast_engine.layers\n",
    "        future_predictions = self.forecast_engine.futurePredictions\n",
    "\n",
    "        # Extract parameters from the scaler\n",
    "        headroom = self.scaler.headroom\n",
    "\n",
    "        # Invert transformations\n",
    "        # train_predictions_inverted = self.invert_predictions(train_predictions)\n",
    "        # historical_data_inverted = self.invert_predictions(curr_dataset) -> curr_dataset was never transformed\n",
    "        # wrapping train_Y into a list -> to make it 2D, which invert_predictions requires\n",
    "        # train_Y_inverted = self.invert_predictions([train_Y])\n",
    "        # future_predictions_inverted = self.invert_predictions(future_predictions)\n",
    "\n",
    "        # Calculate RMSE (Of the Training prediction, not forecast)\n",
    "        # train_rmse = self.calculate_rmse(historical_data_inverted[look_back:], train_predictions_inverted)\n",
    "        # train_rmse = self.calculate_rmse(train_Y_inverted[0], train_predictions_inverted[:, 0])\n",
    "        # print('Train Score: %.2f RMSE' % (train_rmse))\n",
    "\n",
    "        # Create full time array for x-axis\n",
    "        # full_time = np.arange(len(curr_dataset) + len(future_predictions_inverted))\n",
    "        full_time = np.arange(len(future_predictions_inverted))\n",
    "        # full_time = np.arange(train_end + len(future_predictions_inverted))\n",
    "\n",
    "        # Create plot array for plot values\n",
    "        # plot_array_train = self.create_plot_array(full_time, len(curr_dataset)-len(train_predictions_inverted), train_predictions_inverted)\n",
    "        # plot_array_forecast = self.create_plot_array(full_time, train_end, future_predictions_inverted)\n",
    "        plot_array_forecast = self.create_plot_array(full_time, 0, future_predictions_inverted)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # plt.plot(full_time[:len(curr_dataset)], curr_dataset, color='blue', linewidth=1.5, label='Given Data (TRAIN)') \n",
    "        # plt.plot(full_time[train_end:test_end], curr_dataset[train_end:test_end], color='green', linewidth=1.5, alpha=0.95, label='Given Data (TEST)')\n",
    "        # plt.plot(full_time[train_end:test_end], plot_array_forecast[train_end:test_end], color='red', linestyle='--', linewidth=1.5, alpha=0.75, label='Future Predictions')\n",
    "        plt.plot(full_time[:], curr_dataset[train_end:test_end, target_col], color='green', linewidth=1.5, alpha=0.95, label='Given Data (TEST)')\n",
    "        plt.plot(full_time[:], plot_array_forecast[:], color='red', linestyle='--', linewidth=1.5, alpha=0.75, label='Future Predictions')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Time (Days)')\n",
    "        plt.ylabel('Value (Points)')\n",
    "        plt.title(f'{curr_system}, RMSE: {rmse:.2f} | TAF A:{TAFvars[0]} B:{TAFvars[1]} W:{TAFvars[2]}')\n",
    "        \n",
    "        plt.legend()\n",
    "\n",
    "        # save_dir = f\"/mnt/slurm_nfs/ece498_w25_20/Stock-Market-Predictor/Model/{curr_dir}/\"\n",
    "        save_dir = f\"{curr_dir}/\"\n",
    "        \n",
    "        plt.savefig(f\"{save_dir}{curr_system}_predictions Lr_{layers} H_{headroom} N_{neurons} B_{batch_size} L_{look_back} E_{epochs} D_{dropout} (TAF A {TAFvars[0]} B {TAFvars[1]} W {TAFvars[2]}).png\")\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Saved plot for column: {curr_system}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tests\n",
    "\n",
    "This code runs the forecasting pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enabling multi-GPU useage on 1 node\n",
    "# gpu_strategy = tf.distribute.MirroredStrategy()\n",
    "# print(f\"Number of GPUs Available: {gpu_strategy.num_replicas_in_sync}\")\n",
    "\n",
    "gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "print(f\"Num GPUs Available: {gpus}\")\n",
    "#print(f\"Worker (1 task per node) {os.environ.get('SLURM_PROCID', 'N/A')} sees {len(gpus)} GPU(s).\")\n",
    "\n",
    "# Tensorflow distributed compute strategy (some shit that makes a distributed environment/rule-set)\n",
    "# gpu_strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# ------------------------------\n",
    "#          Run Script\n",
    "# ------------------------------\n",
    "def run():\n",
    "    \n",
    "    data_dir = 'data/'\n",
    "    results_dir = 'Model/results/'\n",
    "\n",
    "    curr_dir = 'results1_long'\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    # Ensure the CSV is divided into columns named 'System1', 'System2', etc.\n",
    "    # file = 'UTD_Load_sorted.csv'\n",
    "    file = 'Stock Data.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(data_dir+file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file}' not found. Place it into the '/data' directory\")\n",
    "        return\n",
    "\n",
    "# ==================== Global Parameters ====================\n",
    "    feature_cols = ['open',\n",
    "                    'high',\n",
    "                    'low',\n",
    "                    'close',\n",
    "                    'volume',\n",
    "                    'rsi',\n",
    "                    'macd',\n",
    "                    'macdh',\n",
    "                    'macds']\n",
    "    target_feature_col = 3\n",
    "    features = len(feature_cols)\n",
    "    \n",
    "    # Fixed parameters\n",
    "    batch_size = 256\n",
    "    headroom = 1.0\n",
    "    dropout = 0.0\n",
    "    layers = 2\n",
    "    neurons = 100\n",
    "    activation = 'tanh'\n",
    "    # activation = 'relu' # NO Good!\n",
    "    \n",
    "    # Forecasting and dataset parameters\n",
    "    forecast_horizon = 60   # Number of future points to forecast per fold\n",
    "    initial_train_size = 4500\n",
    "    step_size = 0            # For rolling window (0 -> no rolling)\n",
    "\n",
    "    stocks = ['AAPL', 'NVDA']\n",
    "    \n",
    "\n",
    "    for stock in stocks:\n",
    "        if stock != 'AAPL':\n",
    "            continue\n",
    "\n",
    "        curr_dataset = filter_multi_features(df, stock, feature_cols)\n",
    "\n",
    "        # ==================== Bayesian Optimization Setup ====================\n",
    "        # Define the objective function for Bayesian Optimization\n",
    "        def objective(look_back, epochs):\n",
    "            look_back = int(look_back)\n",
    "            epochs = int(epochs)\n",
    "            \n",
    "            model_params = {\n",
    "                'features': features,\n",
    "                'look_back': look_back,\n",
    "                'batch_size': batch_size,\n",
    "                'epochs': epochs,\n",
    "                'headroom': headroom,\n",
    "                'dropout': dropout,\n",
    "                'layers': layers,\n",
    "                'neurons': neurons,\n",
    "                'activation': activation\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Run cross-validation (non-TAF version) and obtain RMSE.\n",
    "                # time_series_cross_validation (FALSE) is expected to return:\n",
    "                # model_components, train_predict_inverted, effective_train_end, test_end, forecasted_inverted, rmse_taf_preTAF\n",
    "                _, _, _, _, _, rmse = time_series_cross_validation(\n",
    "                    curr_dataset, model_params, forecast_horizon, initial_train_size, step_size, target_feature_col, False)\n",
    "            except Exception as e:\n",
    "                print(\"Error during evaluation:\", e)\n",
    "                traceback.print_exc()\n",
    "                rmse = 1e6\n",
    "\n",
    "            # BayesianOptimization maximizes the objective so return negative RMSE.\n",
    "            return -rmse\n",
    "\n",
    "        # *****************************\n",
    "        # Define parameter bounds\n",
    "        pbounds = {\n",
    "            'look_back': (2786, 2786),\n",
    "            'epochs': (348, 348)\n",
    "        }\n",
    "        # *****************************\n",
    "        \n",
    "        optimizer = BayesianOptimization(\n",
    "            f=objective,\n",
    "            pbounds=pbounds,\n",
    "            random_state=42  # For reproducibility\n",
    "        )\n",
    "        \n",
    "        print(\"Starting Bayesian optimization for:\", stock)\n",
    "        optimizer.maximize(\n",
    "            init_points=5,   # Number of random initialization points\n",
    "            n_iter=20        # Number of iterations for the optimization\n",
    "        )\n",
    "        \n",
    "        best_params = optimizer.max['params']\n",
    "        optimal_look_back = int(best_params['look_back'])\n",
    "        optimal_epochs = int(best_params['epochs'])\n",
    "        print(f\"Optimal parameters for {stock}: look_back={optimal_look_back}, epochs={optimal_epochs}\")\n",
    "        \n",
    "        # ==================== Run Cross-Validation with Optimal Parameters ====================\n",
    "        optimal_model_params = {\n",
    "            'features': features,\n",
    "            'look_back': optimal_look_back,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': optimal_epochs,\n",
    "            'headroom': headroom,\n",
    "            'dropout': dropout,\n",
    "            'layers': layers,\n",
    "            'neurons': neurons,\n",
    "            'activation': activation\n",
    "        }\n",
    "        \n",
    "        model, train_data_inverted, train_end, test_end, non_taf_forecast, rmse_non_taf, rmse_TAFs = time_series_cross_validation(curr_dataset, optimal_model_params, forecast_horizon, initial_train_size, step_size, target_feature_col, True)\n",
    "        \n",
    "        visualizer = Visualizer(scaler=model[0].scaler,\n",
    "                                trained_model=model[1],\n",
    "                                forecast_engine=model[2])\n",
    "        visualizer.plot_results(rmse_non_taf, train_data_inverted, train_end, test_end, non_taf_forecast, curr_dataset, stock, target_feature_col, results_dir+curr_dir, [0, 0, 0])\n",
    "        print(\"|=====================================|\")\n",
    "        print(\"Cross-Validation RMSEs (Non-TAF):\", rmse_non_taf)\n",
    "        print(\"|=====================================|\")\n",
    "        for (alpha, beta, weight), (rmse_taf, adjusted_forecast) in rmse_TAFs.items():\n",
    "            visualizer.plot_results(rmse_taf, train_data_inverted, train_end, test_end, adjusted_forecast, curr_dataset, stock, target_feature_col, results_dir+curr_dir, [alpha, beta, weight])\n",
    "            print(\"|=====================================|\")\n",
    "            print(\"Cross-Validation RMSEs (TAF):\", rmse_taf)\n",
    "            print(\"|=====================================|\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We have experimented with  a variety of techniques to reduce RMSE and improve our stock price predictions. We have been able to predict stock prices for 60 days into the future with an RMSE of 1-7.\n",
    "\n",
    "## Techniques\n",
    "\n",
    "### Best LSTM Parameters\n",
    "\n",
    "After optimizing our parameters, we found the best results using 2 layers, 100 neurons, a batch size of 256, and a lookback and number of epochs at 2786 and 348 respectively. Addtionally, we got our best results with the dropout rate set to 0. We have found that the parameters for an LSTM are all very co-dependant on eachother and are best optimized together instead of individually. Especially look back and epochs were heavily dependant on batch size.\n",
    "\n",
    "For hyperparameter optimization, we found Bayesian optimization to be very useful for training the LSTM, mostly for look back and epochs.\n",
    "\n",
    "We found results were best when all features were forcasted in the same step instead of one at a time. Conversely, we found better predictions when foreacting one time step (1 day) at a time instead of all 60 at once.\n",
    "\n",
    "### Trend Adjusted Forecast\n",
    "\n",
    "The TAF was less impactful than we hoped. For many of our trials ended with the TAF adjustments converging to 0, meaning it was not useful. When we used larger batch sizes, we found that the TAF almost never had any effects.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "We found tanh had the better performance for the stock prediction thans relu.\n",
    "\n",
    "### News Embeddings\n",
    "\n",
    "For the news embeddings the results were not very good, though this could be due to the lack in training data for corperate news, as our dataset needed mostly manual collection and had few rows.\n",
    "\n",
    "## Insights on Data Analytics\n",
    "\n",
    "Techniques that work well for other applications did not automatically perform better in our situation (tanh vs relu). It is important to be aware of all of them and see what works in each case.\n",
    "\n",
    "## Recommendations for Future Work\n",
    "\n",
    "Most of our LSTM models suffered from underfitting, perhaps using a more complex architecture with deeper layers could address this. Also, while we could easily collect numerical data on stock, it was much more difficult to collect news data resulting in us having a very small dataset. A potential solution would be to purchase access to this news from one for the many official sources who have years of archive (including OpenBB sources)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
